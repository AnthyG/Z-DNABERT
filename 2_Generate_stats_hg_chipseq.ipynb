{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dumerenkov/anaconda3/lib/python3.7/site-packages/pandas/compat/_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/home/dumerenkov/anaconda3/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "import random\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from tqdm import trange\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import os\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from joblib import Parallel, delayed, dump, load\n",
    "from matplotlib import pyplot as plt\n",
    "from sparse_vector.sparse_vector import SparseVector\n",
    "from scipy.signal import convolve2d, convolve\n",
    "import time\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torch\n",
    "from transformers import BertModel, BertConfig, PreTrainedTokenizer, BasicTokenizer, BertForTokenClassification\n",
    "import collections\n",
    "\n",
    "from transformers import utils\n",
    "from bertviz import model_view, head_view\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dna_tokenizer import DNATokenizer, seq2kmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, chroms, features, \n",
    "                 dna_source, features_source, \n",
    "                 labels_source, intervals, tokenizer):\n",
    "\n",
    "        self.chroms = chroms\n",
    "        self.features = features\n",
    "        self.dna_source = dna_source\n",
    "        self.features_source = features_source\n",
    "        self.labels_source = labels_source\n",
    "        self.intervals = intervals\n",
    "        self.le = LabelBinarizer().fit(np.array([[\"A\"], [\"C\"], [\"T\"], [\"G\"]]))\n",
    "        self.configs = {\n",
    "                        'ZHUNT_AS': {\n",
    "                                'CG': 0, 'GC': 1, 'CA': 0, 'AC': 1, \n",
    "                                'TG': 0, 'GT': 1, 'TA': 1, 'AT': 1, \n",
    "                                'CC': 0, 'GG': 0, 'CT': 1, 'TC': 1, \n",
    "                                'GA': 1, 'AG': 1, 'AA': 1, 'TT': 1},\n",
    "                       }\n",
    "        seqs = ([\"A\", \"C\", \"T\", \"G\"] + \n",
    "                ['AC', 'AT', 'AG', 'CT', 'CG', 'GT'] +\n",
    "                ['AAC', 'ACC', 'AAT', 'ATT', 'AAG', 'AGG', \n",
    "                 'CCA', 'CAA', 'CCT', 'CTT', 'CCG', 'CGG', \n",
    "                 'TTA', 'TAA', 'TTC', 'TCC', 'TTG', 'TGG', \n",
    "                 'GGA', 'GAA', 'GGC', 'GCC', 'GGT', 'GTT'] +\n",
    "                ['AAAC', 'AAAT', 'AAAG', 'CCCA', 'CCCT', 'CCCG',\n",
    "                 'TTTA', 'TTTC', 'TTTG', 'GGGA', 'GGGC', 'GGGT'])\n",
    "        self.tars = np.array([self.le.transform(list(i * 11)[:11]) for i in seqs])[:, ::-1, ::-1]\n",
    "        # purine-pyrimidine\n",
    "        self.tars = np.concatenate((self.tars, np.array([self.tars[4] + self.tars[9]])))\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.intervals)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        interval = self.intervals[index]\n",
    "        chrom = interval[0]\n",
    "        begin = int(interval[1])\n",
    "        end = int(interval[2])\n",
    "        ll = list(self.dna_source[chrom][begin:end].upper())\n",
    "        y = self.labels_source[interval[0]][interval[1]: interval[2]]        \n",
    "        \n",
    "        \n",
    "#         DNA PART\n",
    "        \n",
    "        dna_OHE = self.le.transform(ll)[None]\n",
    "        \n",
    "        res = pd.DataFrame(convolve(dna_OHE, self.tars)[:, 5:-5, 3].T / 11)\n",
    "        res = (res.rolling(5, min_periods=1).max().values == 1).astype(int)\n",
    "        \n",
    "        \n",
    "#         ZHUNT PART\n",
    "        zhunts = []\n",
    "        for key in self.configs:\n",
    "            vec = np.array(ll)\n",
    "            vec = np.vectorize(lambda x:self.configs[key].get(x, 0))(\n",
    "                                    np.char.add(vec[1:], vec[:-1]))\n",
    "            zhunts.append(np.concatenate([vec, [0]]))\n",
    "        \n",
    "        \n",
    "        # FEATURES PART\n",
    "        feature_matr = []\n",
    "        for feature in self.features:\n",
    "            source = self.features_source[feature]\n",
    "            feature_matr.append(source[chrom][begin:end])\n",
    "        \n",
    "        # UNION\n",
    "        if len(feature_matr) > 0:\n",
    "            X = np.hstack((\n",
    "                           res,\n",
    "                           np.array(zhunts).T, \n",
    "                           np.array(feature_matr).T/1000)).astype(np.float32)\n",
    "#             X = (np.array(feature_matr).T/1000).astype(np.float32)\n",
    "        else:\n",
    "            X = dna_OHE.astype(np.float32)\n",
    "        \n",
    "        #K-mer part\n",
    "        \n",
    "        k_mers = seq2kmer(self.dna_source[chrom][begin:end+5].upper(),6)\n",
    "        encoded_k_mers = self.tokenizer.encode_plus(k_mers, add_special_tokens=False, max_length=512)[\"input_ids\"]\n",
    "\n",
    "        return torch.Tensor(X), torch.Tensor(y).long(), ll, torch.LongTensor(encoded_k_mers), (chrom, begin, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                         | 0/2642 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2642/2642 [04:22<00:00, 10.07it/s]\n",
      "  0%|                                                                                                                         | 0/2642 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2642/2642 [01:38<00:00, 26.72it/s]\n",
      "  0%|                                                                                                                         | 0/2642 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2642/2642 [09:18<00:00,  4.73it/s]\n",
      "  0%|                                                                                                                         | 0/2642 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2642/2642 [04:56<00:00,  8.92it/s]\n",
      "  0%|                                                                                                                         | 0/2641 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2641/2641 [01:39<00:00, 26.63it/s]\n"
     ]
    }
   ],
   "source": [
    "attentions, preds, targets, seqs, bps = [],[],[], [], []\n",
    "\n",
    "for FOLD in range(5):\n",
    "    train_dataset, test_dataset = load(f'ds_w_seq_hg_fold{FOLD}.pkl')\n",
    "    model = BertForTokenClassification.from_pretrained(f'dnabert_hg_fold_{FOLD}', output_attentions=True)\n",
    "    \n",
    "    for example in tqdm(test_dataset):\n",
    "        features, target, seq, input_ids, interval = example\n",
    "        if target.numpy().sum()>0:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids.unsqueeze(0))\n",
    "                \n",
    "                pred = torch.softmax(outputs[-2], axis = -1)[0,:,1]\n",
    "                attention = outputs[-1]\n",
    "\n",
    "                \n",
    "            attentions.append(attention)\n",
    "            preds.append(pred)\n",
    "            targets.append(target)\n",
    "            seqs.append(seq)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "629it [05:27,  1.92it/s]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "kmer2pred = defaultdict(int)\n",
    "kmer2att = defaultdict(float)\n",
    "\n",
    "\n",
    "for attention, pred, target, seq in tqdm(zip(attentions, preds, targets, seqs)):\n",
    "    kmer = seq2kmer(''.join(seq), 6).split(' ')\n",
    "    #print(kmer)\n",
    "    att = attention[-1][0,:,:,:]\n",
    "    \n",
    "    for idx in range(512-5):\n",
    "        if target[idx]>0:\n",
    "            kmer2pred[kmer[idx]]+=1\n",
    "            \n",
    "            for head in range(12):\n",
    "                c_att = att[head,idx,:].numpy()\n",
    "                for att_idx in range(512-5):\n",
    "                    kmer2att[kmer[att_idx]]+=c_att[att_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_pred = [t[0] for t in sorted(kmer2pred.items(), key=lambda item: -item[1])]\n",
    "kmer2att2 = {k:int(kmer2att[k]) for k in kmer2att}\n",
    "sorted_att  = [t[0] for t in sorted(kmer2att2.items(), key=lambda item: -item[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 GCGCGC 5\n",
      "2 CGCGCG 6\n",
      "3 TGTGTG 1\n",
      "4 GTGTGT 2\n",
      "5 ACACAC 4\n",
      "6 CACACA 3\n",
      "7 GCACAC 7\n",
      "8 GTGCGC 19\n",
      "9 TGCGCG 12\n",
      "10 GCGCAC 17\n",
      "11 GTGTGC 10\n",
      "12 CGCGCA 14\n",
      "13 CGCACA 8\n",
      "14 GCGCGT 26\n",
      "15 ACGCGC 27\n",
      "16 CGCGTG 32\n",
      "17 TGTGCG 18\n",
      "18 ACACGC 15\n",
      "19 GCGTGC 41\n",
      "20 CACACG 11\n",
      "21 CACGCG 31\n",
      "22 TGCGTG 35\n",
      "23 CGTGTG 25\n",
      "24 GCACGC 39\n",
      "25 GCGTGT 28\n",
      "26 GGCGGC 23\n",
      "27 ACGCAC 24\n",
      "28 TTTTTT 22\n",
      "29 CACGCA 21\n",
      "30 AAAAAA 9\n",
      "31 GCGGCG 20\n",
      "32 GCCGCC 29\n",
      "33 GAGAGA 13\n",
      "34 GTGCGT 36\n",
      "35 CGTGCG 44\n",
      "36 AGAGAG 16\n",
      "37 CTCTCT 46\n",
      "38 CGCCGC 33\n",
      "39 CGGCGG 34\n",
      "40 CCTCCC 42\n",
      "41 TCTCTC 37\n",
      "42 CGCACG 51\n",
      "43 CCGCCC 30\n",
      "44 GGCGGG 43\n",
      "45 GGGCGG 45\n",
      "46 GGGAGG 40\n",
      "47 GCCCGC 78\n",
      "48 CTCCTC 70\n",
      "49 CTCCCC 60\n",
      "50 TCCTCC 76\n",
      "51 GCGGGG 50\n",
      "52 GCCTCC 53\n",
      "53 CCCGCC 38\n",
      "54 CGCCCC 52\n",
      "55 CCCTCC 59\n",
      "56 GGCCGC 56\n",
      "57 CGGCGC 72\n",
      "58 GCGCCC 63\n",
      "59 GCGGCC 55\n",
      "60 CCCCGC 58\n",
      "61 GGCTGC 61\n",
      "62 CCTCCT 57\n",
      "63 GGGGAG 85\n",
      "64 CCGCCG 48\n",
      "65 GCCCCC 107\n",
      "66 GCCCGG 49\n",
      "67 CCCGGC 47\n",
      "68 GCCCCG 66\n",
      "69 CGGGGC 79\n",
      "70 GGGGCG 84\n",
      "71 GCGCGG 75\n",
      "72 GCCGCG 90\n",
      "73 CGCGGC 65\n",
      "74 GGCGCG 87\n",
      "75 GAGGAG 108\n",
      "76 GGGCGC 95\n",
      "77 GGAGGG 86\n",
      "78 GGCCGG 64\n",
      "79 GCCGGG 73\n",
      "80 CGCCCG 69\n",
      "81 GGAGGC 71\n",
      "82 CCCGGG 54\n",
      "83 CTCCGC 131\n",
      "84 GGCCCC 99\n",
      "85 GGCGCC 113\n",
      "86 GCTGGG 74\n",
      "87 GCCTGC 123\n",
      "88 GCCCAG 104\n",
      "89 ATGTGT 265\n",
      "90 CGCGCC 82\n",
      "91 CTCTCC 129\n",
      "92 CCGCGC 89\n",
      "93 GGCCTC 119\n",
      "94 GGCAGC 94\n",
      "95 GCGGAG 155\n",
      "96 AGGCGG 153\n",
      "97 CCCCCC 186\n",
      "98 GCGCCG 124\n",
      "99 CTGGGC 77\n",
      "100 GGGGGC 184\n",
      "101 GCCGGC 197\n",
      "102 CCCGCG 93\n"
     ]
    }
   ],
   "source": [
    "for idx, kmer in enumerate(sorted_att):\n",
    "    print(idx+1, kmer, sorted_pred.index(kmer)+1)\n",
    "    if idx>100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroms = [f'chr{i}' for i in list(range(1, 23)) + ['X', 'Y']]\n",
    "ZDNA = load(f'../data/hg19_zdna/sparse/ZDNA_2016.pkl')\n",
    "black_list = load(f'../data/hg19_zdna/sparse/blacklist_hg19.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [02:50<00:00,  7.12s/it]\n"
     ]
    }
   ],
   "source": [
    "all_pred = []\n",
    "all_true = []\n",
    "for chrom in tqdm(chroms):\n",
    "    all_pred.append(load(f\"/gim/lv01/dumerenkov/zdna_data/new_mod_hg_res_{chrom}\"))\n",
    "    all_true.append(ZDNA[chrom][:].astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9501730035376861"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(np.concatenate(all_true), np.concatenate(all_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9980    0.9990 3095541102\n",
      "           1     0.0105    0.4816    0.0206    136310\n",
      "\n",
      "    accuracy                         0.9980 3095677412\n",
      "   macro avg     0.5052    0.7398    0.5098 3095677412\n",
      "weighted avg     0.9999    0.9980    0.9989 3095677412\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.classification_report(np.concatenate(all_true), np.concatenate(all_pred)>0.5, digits=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
