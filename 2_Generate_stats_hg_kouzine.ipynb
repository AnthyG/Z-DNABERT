{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dumerenkov/anaconda3/lib/python3.7/site-packages/pandas/compat/_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/home/dumerenkov/anaconda3/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "import random\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from tqdm import trange\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import os\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from joblib import Parallel, delayed, dump, load\n",
    "from matplotlib import pyplot as plt\n",
    "from sparse_vector.sparse_vector import SparseVector\n",
    "from scipy.signal import convolve2d, convolve\n",
    "import time\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torch\n",
    "from transformers import BertModel, BertConfig, PreTrainedTokenizer, BasicTokenizer, BertForTokenClassification\n",
    "import collections\n",
    "\n",
    "from transformers import utils\n",
    "from bertviz import model_view, head_view\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dna_tokenizer import DNATokenizer, seq2kmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, chroms, features, \n",
    "                 dna_source, features_source, \n",
    "                 labels_source, intervals, tokenizer):\n",
    "\n",
    "        self.chroms = chroms\n",
    "        self.features = features\n",
    "        self.dna_source = dna_source\n",
    "        self.features_source = features_source\n",
    "        self.labels_source = labels_source\n",
    "        self.intervals = intervals\n",
    "        self.le = LabelBinarizer().fit(np.array([[\"A\"], [\"C\"], [\"T\"], [\"G\"]]))\n",
    "        self.configs = {\n",
    "                        'ZHUNT_AS': {\n",
    "                                'CG': 0, 'GC': 1, 'CA': 0, 'AC': 1, \n",
    "                                'TG': 0, 'GT': 1, 'TA': 1, 'AT': 1, \n",
    "                                'CC': 0, 'GG': 0, 'CT': 1, 'TC': 1, \n",
    "                                'GA': 1, 'AG': 1, 'AA': 1, 'TT': 1},\n",
    "                       }\n",
    "        seqs = ([\"A\", \"C\", \"T\", \"G\"] + \n",
    "                ['AC', 'AT', 'AG', 'CT', 'CG', 'GT'] +\n",
    "                ['AAC', 'ACC', 'AAT', 'ATT', 'AAG', 'AGG', \n",
    "                 'CCA', 'CAA', 'CCT', 'CTT', 'CCG', 'CGG', \n",
    "                 'TTA', 'TAA', 'TTC', 'TCC', 'TTG', 'TGG', \n",
    "                 'GGA', 'GAA', 'GGC', 'GCC', 'GGT', 'GTT'] +\n",
    "                ['AAAC', 'AAAT', 'AAAG', 'CCCA', 'CCCT', 'CCCG',\n",
    "                 'TTTA', 'TTTC', 'TTTG', 'GGGA', 'GGGC', 'GGGT'])\n",
    "        self.tars = np.array([self.le.transform(list(i * 11)[:11]) for i in seqs])[:, ::-1, ::-1]\n",
    "        # purine-pyrimidine\n",
    "        self.tars = np.concatenate((self.tars, np.array([self.tars[4] + self.tars[9]])))\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.intervals)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        interval = self.intervals[index]\n",
    "        chrom = interval[0]\n",
    "        begin = int(interval[1])\n",
    "        end = int(interval[2])\n",
    "        ll = list(self.dna_source[chrom][begin:end].upper())\n",
    "        y = self.labels_source[interval[0]][interval[1]: interval[2]]        \n",
    "        \n",
    "        \n",
    "#         DNA PART\n",
    "        \n",
    "        dna_OHE = self.le.transform(ll)[None]\n",
    "        \n",
    "        res = pd.DataFrame(convolve(dna_OHE, self.tars)[:, 5:-5, 3].T / 11)\n",
    "        res = (res.rolling(5, min_periods=1).max().values == 1).astype(int)\n",
    "        \n",
    "        \n",
    "#         ZHUNT PART\n",
    "        zhunts = []\n",
    "        for key in self.configs:\n",
    "            vec = np.array(ll)\n",
    "            vec = np.vectorize(lambda x:self.configs[key].get(x, 0))(\n",
    "                                    np.char.add(vec[1:], vec[:-1]))\n",
    "            zhunts.append(np.concatenate([vec, [0]]))\n",
    "        \n",
    "        \n",
    "        # FEATURES PART\n",
    "        feature_matr = []\n",
    "        for feature in self.features:\n",
    "            source = self.features_source[feature]\n",
    "            feature_matr.append(source[chrom][begin:end])\n",
    "        \n",
    "        # UNION\n",
    "        if len(feature_matr) > 0:\n",
    "            X = np.hstack((\n",
    "                           res,\n",
    "                           np.array(zhunts).T, \n",
    "                           np.array(feature_matr).T/1000)).astype(np.float32)\n",
    "#             X = (np.array(feature_matr).T/1000).astype(np.float32)\n",
    "        else:\n",
    "            X = dna_OHE.astype(np.float32)\n",
    "        \n",
    "        #K-mer part\n",
    "        \n",
    "        k_mers = seq2kmer(self.dna_source[chrom][begin:end+5].upper(),6)\n",
    "        encoded_k_mers = self.tokenizer.encode_plus(k_mers, add_special_tokens=False, max_length=512)[\"input_ids\"]\n",
    "\n",
    "        return torch.Tensor(X), torch.Tensor(y).long(), ll, torch.LongTensor(encoded_k_mers), (chrom, begin, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                        | 0/17847 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 17847/17847 [10:40<00:00, 27.87it/s]\n",
      "5949it [06:19, 15.68it/s]\n",
      "  0%|                                                                                                                        | 0/17847 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 17847/17847 [10:27<00:00, 28.45it/s]\n",
      "5949it [06:23, 15.50it/s]\n",
      "  0%|                                                                                                                        | 0/17847 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 17847/17847 [11:32<00:00, 25.78it/s]\n",
      "5949it [06:40, 14.86it/s]\n",
      "  0%|                                                                                                                        | 0/17847 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 17847/17847 [11:35<00:00, 25.66it/s]\n",
      "5949it [06:26, 15.39it/s]\n",
      "  0%|                                                                                                                        | 0/17847 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 17847/17847 [11:28<00:00, 25.91it/s]\n",
      "5949it [06:25, 15.43it/s]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "kmer2pred = defaultdict(int)\n",
    "kmer2att = defaultdict(float)\n",
    "\n",
    "device = 1\n",
    "for FOLD in range(5):\n",
    "    gc.collect()\n",
    "    attentions, preds, targets, seqs, bps = [],[],[], [], []\n",
    "    train_dataset, test_dataset = load(f'/gim/lv01/dumerenkov/zdna_data/datasets/ds_w_seq_hg_fold{FOLD}_kouzine.pkl')\n",
    "    model = BertForTokenClassification.from_pretrained(f'dnabert_hg_fold_{FOLD}_kouzine', output_attentions=True)\n",
    "    model.to(device)\n",
    "    for example in tqdm(test_dataset):\n",
    "        features, target, seq, input_ids, interval = example\n",
    "        if target.numpy().sum()>0:\n",
    "            with torch.no_grad():\n",
    "                input_ids = input_ids.to(device)\n",
    "                outputs = model(input_ids.unsqueeze(0))\n",
    "                \n",
    "                raw_preds = outputs[-2].detach().to('cpu')\n",
    "                attention = outputs[-1][-1].detach().to('cpu')\n",
    "                \n",
    "                pred = torch.softmax(raw_preds, axis = -1)[0,:,1]\n",
    "\n",
    "                \n",
    "            attentions.append(attention)\n",
    "            preds.append(pred)\n",
    "            targets.append(target)\n",
    "            seqs.append(seq)\n",
    "            \n",
    "    for attention, pred, target, seq in tqdm(zip(attentions, preds, targets, seqs)):\n",
    "        kmer = seq2kmer(''.join(seq), 6).split(' ')\n",
    "        #print(kmer)\n",
    "        att = attention[0,:,:,:]\n",
    "    \n",
    "        for idx in range(512-5):\n",
    "            if target[idx]>0:\n",
    "                kmer2pred[kmer[idx]]+=1\n",
    "            \n",
    "                for head in range(12):\n",
    "                    c_att = att[head,idx,:].numpy()\n",
    "                    for att_idx in range(512-5):\n",
    "                        kmer2att[kmer[att_idx]]+=c_att[att_idx]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_pred = [t[0] for t in sorted(kmer2pred.items(), key=lambda item: -item[1])]\n",
    "kmer2att2 = {k:int(kmer2att[k]) for k in kmer2att}\n",
    "sorted_att  = [t[0] for t in sorted(kmer2att2.items(), key=lambda item: -item[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 GCGCGC 1\n",
      "2 GTGTGT 5\n",
      "3 CGCGCG 2\n",
      "4 ACACAC 6\n",
      "5 TGTGTG 3\n",
      "6 GCGCGG 7\n",
      "7 CACACA 4\n",
      "8 CCGCGC 10\n",
      "9 GGGCGC 11\n",
      "10 GCGCCC 12\n",
      "11 GTGCGC 17\n",
      "12 GGCGCG 9\n",
      "13 GTGTGC 14\n",
      "14 GCGCAC 19\n",
      "15 GCACAC 15\n",
      "16 GCCCGC 20\n",
      "17 GCGGGC 16\n",
      "18 CGCGCC 8\n",
      "19 GCGTGC 25\n",
      "20 GCACGC 26\n",
      "21 CCCGCG 18\n",
      "22 CGCGGG 13\n",
      "23 GTGCAC 31\n",
      "24 CGGGCG 24\n",
      "25 ACGCGC 35\n",
      "26 GCGCGT 34\n",
      "27 CGCCCG 23\n",
      "28 TGCGCG 22\n",
      "29 CGCGCA 21\n",
      "30 CGTGTG 27\n",
      "31 GCGTGT 39\n",
      "32 GTGCGT 47\n",
      "33 CACGCG 28\n",
      "34 ACACGC 44\n",
      "35 CGCGTG 29\n",
      "36 CGTGCG 32\n",
      "37 ACGCAC 53\n",
      "38 CACACG 30\n",
      "39 CGCACG 33\n",
      "40 GCGCAG 48\n",
      "41 GCGGGG 46\n",
      "42 TGTGCG 36\n",
      "43 CTGCGC 52\n",
      "44 TGCGTG 37\n",
      "45 TGCACA 40\n",
      "46 CGCACA 38\n",
      "47 GCGCCG 51\n",
      "48 GCCGCG 50\n",
      "49 TGTGCA 42\n",
      "50 CCCCGC 59\n",
      "51 CGGCGC 55\n",
      "52 CACGCA 41\n",
      "53 CACGTG 43\n",
      "54 GGGCGG 70\n",
      "55 CGCGGC 45\n",
      "56 GGGGCG 61\n",
      "57 AGGCGC 81\n",
      "58 ACGTGC 79\n",
      "59 GCGGCG 80\n",
      "60 CGCCCC 49\n",
      "61 CCGCCC 71\n",
      "62 GCGCCT 69\n",
      "63 GCACGT 73\n",
      "64 AGCGCG 58\n",
      "65 ATGCAC 101\n",
      "66 TGCACG 57\n",
      "67 GCGTGG 76\n",
      "68 GTGCAT 112\n",
      "69 GGCGGG 64\n",
      "70 GCAGGC 85\n",
      "71 GCCTGC 84\n",
      "72 GTGCGG 89\n",
      "73 CCGCAC 92\n",
      "74 CGCCGC 65\n",
      "75 CGTGCA 56\n",
      "76 GGGCAC 106\n",
      "77 CCACGC 90\n",
      "78 GCGCTC 87\n",
      "79 CCCGCC 60\n",
      "80 GTGGGC 99\n",
      "81 GTGCCC 96\n",
      "82 GAGCGC 95\n",
      "83 CGCGCT 54\n",
      "84 GCCCAC 105\n",
      "85 GGTGCG 77\n",
      "86 CCTGCG 78\n",
      "87 GCATGC 93\n",
      "88 GCCCCG 114\n",
      "89 GCCCGG 116\n",
      "90 GGGTGC 103\n",
      "91 ACACGT 120\n",
      "92 GGCGCA 74\n",
      "93 GCACCC 98\n",
      "94 GGCGTG 86\n",
      "95 CCGCGG 117\n",
      "96 GCGGCC 108\n",
      "97 ACGTGT 119\n",
      "98 CGCAGG 68\n",
      "99 CATGCA 67\n",
      "100 ACGCGG 129\n",
      "101 GCACGG 115\n",
      "102 ACACAT 149\n"
     ]
    }
   ],
   "source": [
    "for idx, kmer in enumerate(sorted_att):\n",
    "    print(idx+1, kmer, sorted_pred.index(kmer)+1)\n",
    "    if idx>100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroms = [f'chr{i}' for i in list(range(1, 23)) + ['X', 'Y']]\n",
    "ZDNA = load('ZDNA_hg19_kouzine.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [02:30<00:00,  6.29s/it]\n"
     ]
    }
   ],
   "source": [
    "all_pred = []\n",
    "all_true = []\n",
    "for chrom in tqdm(chroms):\n",
    "    all_pred.append(load(f\"/gim/lv01/dumerenkov/zdna_data/new_mod_hg_res_{chrom}_kouzine\"))\n",
    "    all_true.append(ZDNA[chrom][:].astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [03:19<00:00,  8.32s/it]\n"
     ]
    }
   ],
   "source": [
    "chroms = [f'chr{i}' for i in list(range(1, 23)) + ['X', 'Y']]\n",
    "ZDNA = load('ZDNA_hg19_kouzine.pkl')\n",
    "black_list = load(f'../data/hg19_zdna/sparse/blacklist_hg19.pkl')\n",
    "\n",
    "all_pred = []\n",
    "all_true = []\n",
    "for chrom in tqdm(chroms):\n",
    "    true_clean = ZDNA[chrom][:].astype(int)\n",
    "    iids = np.where(black_list[chrom].data == 1)[0]\n",
    "    for i, j in zip(black_list[chrom].indices[iids], black_list[chrom].indices[iids + 1]):\n",
    "        true_clean[i:j] = 0\n",
    "    all_pred.append(load(f\"/gim/lv01/dumerenkov/zdna_data/new_mod_hg_res_{chrom}_kouzine\"))\n",
    "    all_true.append(true_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9991730742363316"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(np.concatenate(all_true), np.concatenate(all_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9999    0.9986    0.9993 3094878438\n",
      "           1     0.1185    0.7303    0.2040    798974\n",
      "\n",
      "    accuracy                         0.9985 3095677412\n",
      "   macro avg     0.5592    0.8644    0.6016 3095677412\n",
      "weighted avg     0.9997    0.9985    0.9991 3095677412\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.classification_report(np.concatenate(all_true), np.concatenate(all_pred)>0.5, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
