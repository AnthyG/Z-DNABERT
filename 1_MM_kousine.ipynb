{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dumerenkov/anaconda3/lib/python3.7/site-packages/pandas/compat/_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/home/dumerenkov/anaconda3/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "import random\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from tqdm import trange\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import os\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from joblib import Parallel, delayed, dump, load\n",
    "from matplotlib import pyplot as plt\n",
    "from sparse_vector.sparse_vector import SparseVector\n",
    "from scipy.signal import convolve2d, convolve\n",
    "import time\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ZDNA_kouzine = {}\n",
    "#ZDNA = load(f'../data/mm9_zdna/sparse/curax_14h_UNI_mm9.pkl')\n",
    "#for chrom in ZDNA:\n",
    "#    ZDNA_kouzine[chrom] = np.zeros(ZDNA[chrom].shape, dtype = bool)\n",
    "#    \n",
    "#    \n",
    "#with open(\"Kouzine_mm9.bed\")as f:\n",
    "#    for idx, line in enumerate(f):\n",
    "#        if idx>0:\n",
    "#            chrom, start, end = line.strip().split()\n",
    "#            if chrom in ZDNA_kouzine:\n",
    "#                ZDNA_kouzine[chrom][int(start):int(end)] = 1\n",
    "#\n",
    "#dump(ZDNA_kouzine, 'ZDNA_mm9_kouzine.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASSEMBLY_d = {}\n",
    "chroms_d = {}\n",
    "all_features_d = {}\n",
    "groups_d = {}\n",
    "feature_names_d = {}\n",
    "ZDNA_d = {}\n",
    "black_list_d = {}\n",
    "DNA_d = {}\n",
    "DNA_features_d = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MM9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASSEMBLY = \"curax_14h_UNI_mm9\"\n",
    "chroms = [f'chr{i}' for i in list(range(1, 20)) + ['X', 'Y']]\n",
    "all_features = sorted([i[:-4] for i in os.listdir('../data/mm9_features/sparse/') if i.endswith('.pkl')])\n",
    "groups = ['DNase-seq', 'Histone', 'RNA polymerase', 'TFs and others']\n",
    "feature_names = [i for i in all_features if (i.split('_')[0] in groups)]\n",
    "ZDNA = load(f'ZDNA_mm9_kouzine.pkl')\n",
    "black_list = load(f'../data/mm9_zdna/sparse/blacklist_mm9.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [00:03<00:00,  5.73it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 873/873 [00:20<00:00, 41.76it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 873/873 [00:00<00:00, 231367.84it/s]\n"
     ]
    }
   ],
   "source": [
    "DNA = {chrom:load(f'../data/mm9_dna/sparse/{chrom}.pkl') for chrom in tqdm(chroms)}\n",
    "\n",
    "DNA_features = {feture: load(f'../data/mm9_features/sparse/{feture}.pkl')\n",
    "                for feture in tqdm(feature_names)}\n",
    "\n",
    "for feature in tqdm(DNA_features):\n",
    "    if set(DNA_features[feature].keys()) != set(chroms):\n",
    "        for chrom in chroms:\n",
    "            if chrom not in DNA_features[feature]:\n",
    "                DNA_features[feature][chrom] = SparseVector(len(DNA[chrom]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'mm9'\n",
    "ASSEMBLY_d[mode] = ASSEMBLY\n",
    "chroms_d[mode] = chroms\n",
    "all_features_d[mode] = all_features\n",
    "groups_d[mode] = groups\n",
    "feature_names_d[mode] = feature_names\n",
    "ZDNA_d[mode] = ZDNA\n",
    "black_list_d[mode] = black_list\n",
    "DNA_d[mode] = DNA\n",
    "DNA_features_d[mode] = DNA_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASSEMBLY = \"ZDNA_2016\"\n",
    "chroms = [f'chr{i}' for i in list(range(1, 23)) + ['X', 'Y']]\n",
    "all_features = sorted([i[:-4] for i in os.listdir('../data/hg19_features/sparse/') if i.endswith('.pkl')])\n",
    "groups = ['DNase-seq', 'Histone', 'RNA polymerase', 'TFs and others']\n",
    "feature_names = [i for i in all_features if (i.split('_')[0] in groups)]\n",
    "ZDNA = load('ZDNA_hg19_kouzine.pkl')\n",
    "black_list = load(f'../data/hg19_zdna/sparse/blacklist_hg19.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:52<00:00,  2.19s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1054/1054 [00:58<00:00, 17.95it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 1054/1054 [00:00<00:00, 141041.23it/s]\n"
     ]
    }
   ],
   "source": [
    "def chrom_reader(chrom):\n",
    "    files = sorted([i for i in os.listdir(f'../data/hg19_dna/') if f\"{chrom}_\" in i])\n",
    "    return ''.join([load(f\"../data/hg19_dna/{file}\") for file in files])\n",
    "\n",
    "\n",
    "DNA = {chrom:chrom_reader(chrom) for chrom in tqdm(chroms)}\n",
    "\n",
    "DNA_features = {feture: load(f'../data/hg19_features/sparse/{feture}.pkl')\n",
    "                for feture in tqdm(feature_names)}\n",
    "\n",
    "for feature in tqdm(DNA_features):\n",
    "    if set(DNA_features[feature].keys()) != set(chroms):\n",
    "        for chrom in chroms:\n",
    "            if chrom not in DNA_features[feature]:\n",
    "                DNA_features[feature][chrom] = SparseVector(len(DNA[chrom]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'hg19'\n",
    "ASSEMBLY_d[mode] = ASSEMBLY\n",
    "chroms_d[mode] = chroms\n",
    "all_features_d[mode] = all_features\n",
    "groups_d[mode] = groups\n",
    "feature_names_d[mode] = feature_names\n",
    "ZDNA_d[mode] = ZDNA\n",
    "black_list_d[mode] = black_list\n",
    "DNA_d[mode] = DNA\n",
    "DNA_features_d[mode] = DNA_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersect = {i.upper() for i in DNA_features_d['mm9']} & {i.upper() for i in DNA_features_d['hg19']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_bed(source, name):\n",
    "    buf = []\n",
    "    for chrm in source[name]:\n",
    "        beds = source[name][chrm].indices[source[name][chrm].data != 0]\n",
    "        data = source[name][chrm].data[source[name][chrm].data != 0].astype(float).astype(int)\n",
    "        ends = np.append(source[name][chrm].indices[1:], \n",
    "              [source[name][chrm].shape])[source[name][chrm].data != 0]\n",
    "\n",
    "        buf.extend([[chrm, beds[i], ends[i], data[i]] for i in range(len(beds))])\n",
    "    buf = np.array(buf)\n",
    "    df = pd.DataFrame([buf[:, 0], buf[:, 1], buf[:, 2], '', buf[:, 3], \"\", buf[:, 1], buf[:, 2], \"\"]).T\n",
    "    df[3] = ''\n",
    "    df[5] = \"+\"\n",
    "    df[8] = ''\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gen = 'hg19'\n",
    "#\n",
    "#for name in tqdm([i for i in DNA_features_d[gen] if i.upper() in intersect and 'TFs and others' in i]):\n",
    "#    df = to_bed(DNA_features_d[gen], name)\n",
    "#    df.to_csv(f'tmp/{gen}_{name}.bed', sep = '\\t', index=False, header=None)\n",
    "##     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(DNA_features_d, name, gen):\n",
    "    df = to_bed(DNA_features_d, name)\n",
    "    df.to_csv(f'tmp/{gen}_{name}.bed', sep = '\\t', index=False, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from joblib import Parallel, delayed\n",
    "#gen = 'hg19'\n",
    "#Parallel(n_jobs = -1)(delayed(func)(DNA_features_d[gen], name, gen)\n",
    "#                     for name in tqdm([i for i in DNA_features_d[gen] \n",
    "#                                       if i.upper() in intersect and 'TFs and others' in i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'mm9'\n",
    "ASSEMBLY = ASSEMBLY_d[mode]\n",
    "chroms = chroms_d[mode]\n",
    "all_features = all_features_d[mode]\n",
    "groups = groups_d[mode]\n",
    "feature_names = feature_names_d[mode]\n",
    "ZDNA = ZDNA_d[mode]\n",
    "black_list = black_list_d[mode]\n",
    "DNA = DNA_d[mode]\n",
    "DNA_features = DNA_features_d[mode]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 385147/385147 [00:14<00:00, 26202.84it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 354976/354976 [00:12<00:00, 27423.50it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 311718/311718 [00:11<00:00, 26415.89it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 303965/303965 [00:11<00:00, 27093.68it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 297924/297924 [00:11<00:00, 26954.01it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 292025/292025 [00:10<00:00, 26608.65it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 297899/297899 [00:11<00:00, 26951.02it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 257302/257302 [00:09<00:00, 27971.39it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 242336/242336 [00:09<00:00, 26572.06it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 253893/253893 [00:09<00:00, 27998.77it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 237976/237976 [00:08<00:00, 26999.79it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 236831/236831 [00:08<00:00, 26530.64it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 234930/234930 [00:09<00:00, 25462.39it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 244521/244521 [00:09<00:00, 27105.33it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 202138/202138 [00:07<00:00, 27122.60it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 192029/192029 [00:08<00:00, 23983.76it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 186079/186079 [00:07<00:00, 24168.68it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 177289/177289 [00:07<00:00, 22627.03it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 119809/119809 [00:04<00:00, 26354.00it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 325488/325488 [00:13<00:00, 24204.48it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 31059/31059 [00:01<00:00, 25283.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17017\n",
      "4843301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "width = 512\n",
    "\n",
    "np.random.seed(10)\n",
    "\n",
    "ints_in = []\n",
    "ints_out = []\n",
    "\n",
    "\n",
    "for chrm in chroms:\n",
    "    for st in trange(0, ZDNA[chrm].shape[0] - width, width):\n",
    "        interval = [st, min(st + width, ZDNA[chrm].shape[0])]\n",
    "        N_count = sum([bp == \"N\" for bp in DNA[chrm][interval[0]:interval[1]]])\n",
    "        bl_count = black_list[chrm][interval[0]:interval[1]].sum()\n",
    "        if N_count > width / 2 or bl_count > 0:\n",
    "            continue\n",
    "        else:\n",
    "            if ZDNA[chrm][interval[0]: interval[1]].any():\n",
    "                ints_in.append([chrm, int(interval[0]), int(interval[1]), 1])\n",
    "            else:\n",
    "                ints_out.append([chrm, int(interval[0]), int(interval[1]), 0])\n",
    "\n",
    "\n",
    "                \n",
    "                \n",
    "print(len(ints_in))\n",
    "print(len(ints_out))\n",
    "\n",
    "ints_in_full = ints_in\n",
    "ints_out_full = ints_out\n",
    "#29745\n",
    "#5443119"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17017\n",
      "34034\n"
     ]
    }
   ],
   "source": [
    "ints_in = ints_in_full\n",
    "ints_out = [ints_out_full[i] for i in np.random.choice(range(len(ints_out_full)), \n",
    "                                                    size=len(ints_in) * 2, replace=False)]\n",
    "# ints_out = ints_out_full\n",
    "\n",
    "print(len(ints_in))\n",
    "print(len(ints_out))\n",
    "#484 for len 1000 \n",
    "#9680 for len 1000\n",
    "\n",
    "#629 for len 512\n",
    "#12580 for len 5121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "equalized = ints_in + ints_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "divisions = list(StratifiedKFold(5, shuffle=True, \n",
    "                                 random_state=42).split(equalized, [f\"{elem[3]}_{elem[0]}\"\n",
    "                                         for i, elem \n",
    "                                         in enumerate(equalized)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mm_divisions_kouzine.pkl']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump([equalized, divisions], 'mm_divisions_kouzine.pkl', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, chroms, features, \n",
    "                 dna_source, features_source, \n",
    "                 labels_source, intervals, tokenizer):\n",
    "\n",
    "        self.chroms = chroms\n",
    "        self.features = features\n",
    "        self.dna_source = dna_source\n",
    "        self.features_source = features_source\n",
    "        self.labels_source = labels_source\n",
    "        self.intervals = intervals\n",
    "        self.le = LabelBinarizer().fit(np.array([[\"A\"], [\"C\"], [\"T\"], [\"G\"]]))\n",
    "        self.configs = {\n",
    "                        'ZHUNT_AS': {\n",
    "                                'CG': 0, 'GC': 1, 'CA': 0, 'AC': 1, \n",
    "                                'TG': 0, 'GT': 1, 'TA': 1, 'AT': 1, \n",
    "                                'CC': 0, 'GG': 0, 'CT': 1, 'TC': 1, \n",
    "                                'GA': 1, 'AG': 1, 'AA': 1, 'TT': 1},\n",
    "                       }\n",
    "        seqs = ([\"A\", \"C\", \"T\", \"G\"] + \n",
    "                ['AC', 'AT', 'AG', 'CT', 'CG', 'GT'] +\n",
    "                ['AAC', 'ACC', 'AAT', 'ATT', 'AAG', 'AGG', \n",
    "                 'CCA', 'CAA', 'CCT', 'CTT', 'CCG', 'CGG', \n",
    "                 'TTA', 'TAA', 'TTC', 'TCC', 'TTG', 'TGG', \n",
    "                 'GGA', 'GAA', 'GGC', 'GCC', 'GGT', 'GTT'] +\n",
    "                ['AAAC', 'AAAT', 'AAAG', 'CCCA', 'CCCT', 'CCCG',\n",
    "                 'TTTA', 'TTTC', 'TTTG', 'GGGA', 'GGGC', 'GGGT'])\n",
    "        self.tars = np.array([self.le.transform(list(i * 11)[:11]) for i in seqs])[:, ::-1, ::-1]\n",
    "        # purine-pyrimidine\n",
    "        self.tars = np.concatenate((self.tars, np.array([self.tars[4] + self.tars[9]])))\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.intervals)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        interval = self.intervals[index]\n",
    "        chrom = interval[0]\n",
    "        begin = int(interval[1])\n",
    "        end = int(interval[2])\n",
    "        ll = list(self.dna_source[chrom][begin:end].upper())\n",
    "        y = self.labels_source[interval[0]][interval[1]: interval[2]]        \n",
    "        \n",
    "        \n",
    "#         DNA PART\n",
    "        \n",
    "        dna_OHE = self.le.transform(ll)[None]\n",
    "        \n",
    "        res = pd.DataFrame(convolve(dna_OHE, self.tars)[:, 5:-5, 3].T / 11)\n",
    "        res = (res.rolling(5, min_periods=1).max().values == 1).astype(int)\n",
    "        \n",
    "        \n",
    "#         ZHUNT PART\n",
    "        zhunts = []\n",
    "        for key in self.configs:\n",
    "            vec = np.array(ll)\n",
    "            vec = np.vectorize(lambda x:self.configs[key].get(x, 0))(\n",
    "                                    np.char.add(vec[1:], vec[:-1]))\n",
    "            zhunts.append(np.concatenate([vec, [0]]))\n",
    "        \n",
    "        \n",
    "        # FEATURES PART\n",
    "        feature_matr = []\n",
    "        for feature in self.features:\n",
    "            source = self.features_source[feature]\n",
    "            feature_matr.append(source[chrom][begin:end])\n",
    "        \n",
    "        # UNION\n",
    "        if len(feature_matr) > 0:\n",
    "            X = np.hstack((\n",
    "                           res,\n",
    "                           np.array(zhunts).T, \n",
    "                           np.array(feature_matr).T/1000)).astype(np.float32)\n",
    "#             X = (np.array(feature_matr).T/1000).astype(np.float32)\n",
    "        else:\n",
    "            X = dna_OHE.astype(np.float32)\n",
    "        \n",
    "        #K-mer part\n",
    "        \n",
    "        k_mers = seq2kmer(self.dna_source[chrom][begin:end+5].upper(),6)\n",
    "        encoded_k_mers = self.tokenizer.encode_plus(k_mers, add_special_tokens=False, max_length=512)[\"input_ids\"]\n",
    "\n",
    "        return torch.Tensor(X), torch.Tensor(y).long(), ll, torch.LongTensor(encoded_k_mers), (chrom, begin, end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertConfig, PreTrainedTokenizer, BasicTokenizer, BertForTokenClassification\n",
    "import collections\n",
    "from torch.utils.data import DataLoader\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dna_tokenizer import DNATokenizer, seq2kmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'DNATokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'DNATokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'DNATokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'DNATokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'DNATokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'DNATokenizer'.\n"
     ]
    }
   ],
   "source": [
    "for MODEL_NUMBER in range(5):\n",
    "\n",
    "    train_inds, test_inds = divisions[MODEL_NUMBER]\n",
    "    train_intervals, test_intervals = [equalized[i] for i in train_inds], [equalized[i] for i in test_inds]\n",
    "\n",
    "    random.shuffle(train_intervals)\n",
    "    random.shuffle(test_intervals)\n",
    "    \n",
    "    train_dataset = Dataset(chroms, \n",
    "                        [i for i in feature_names if i.upper() in intersect], \n",
    "                       DNA, DNA_features, ZDNA, train_intervals, \n",
    "                        DNATokenizer.from_pretrained('6-new-12w-0/', add_special_tokens=False))\n",
    "\n",
    "    test_dataset = Dataset(chroms, \n",
    "                       [i for i in feature_names if i.upper() in intersect], \n",
    "                       DNA, DNA_features, ZDNA, test_intervals,\n",
    "                          DNATokenizer.from_pretrained('6-new-12w-0/', add_special_tokens=False))\n",
    "    \n",
    "    dump((train_dataset, test_dataset), f'ds_w_seq_mm_fold{MODEL_NUMBER}_kouzine.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    tr_loss, tr_accuracy = 0, 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    tr_preds, tr_labels = [], []\n",
    "    # put model in training mode\n",
    "    model.train()\n",
    "    \n",
    "    for idx, batch in enumerate(training_loader):\n",
    "        features, labels, sequences, input_ids, intervals = batch            \n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device)\n",
    "        #print(model.device, input_ids.device, labels.device)\n",
    "        outputs = model(input_ids = input_ids, labels = labels)        \n",
    "        #print(outputs)\n",
    "        loss, tr_logits = outputs['loss'], outputs['logits']\n",
    "        #print(model(input_ids=ids, attention_mask=mask, labels=labels))\n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples += labels.size(0)\n",
    "        \n",
    "        if idx % 1000==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            print(f\"Training loss per 1000 training steps: {loss_step}\")\n",
    "           \n",
    "        # compute training accuracy\n",
    "        flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n",
    "        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "        \n",
    "        # only compute accuracy at active labels\n",
    "        active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
    "        active_labels = torch.where(active_accuracy, labels.view(-1), torch.tensor(-100).type_as(labels))\n",
    "        \n",
    "        labels = torch.masked_select(flattened_targets, active_accuracy)\n",
    "        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "        \n",
    "        tr_labels.extend(labels)\n",
    "        tr_preds.extend(predictions)\n",
    "\n",
    "        tmp_tr_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "        tr_accuracy += tmp_tr_accuracy\n",
    "    \n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            parameters=model.parameters(), max_norm=0.1\n",
    "        )\n",
    "        \n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    epoch_loss = tr_loss / nb_tr_steps\n",
    "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
    "    print(f\"Training loss epoch: {epoch_loss}\")\n",
    "    print(f\"Training accuracy epoch: {tr_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, testing_loader):\n",
    "    # put model in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_examples, nb_eval_steps = 0, 0\n",
    "    eval_preds, eval_labels, eval_scores = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(testing_loader):\n",
    "            \n",
    "            features, labels, sequences, input_ids, intervals = batch            \n",
    "            input_ids = input_ids.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(input_ids = input_ids, labels = labels)\n",
    "            loss, eval_logits = outputs['loss'], outputs['logits']            \n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            nb_eval_steps += 1\n",
    "            nb_eval_examples += labels.size(0)\n",
    "        \n",
    "            if idx % 100==0:\n",
    "                loss_step = eval_loss/nb_eval_steps\n",
    "                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n",
    "              \n",
    "            # compute evaluation accuracy\n",
    "            flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n",
    "            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "            flattened_scores = active_logits[:,1] - active_logits[:,0]\n",
    "            \n",
    "            # only compute accuracy at active labels\n",
    "            active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
    "        \n",
    "            labels = torch.masked_select(flattened_targets, active_accuracy)\n",
    "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "            \n",
    "            eval_labels.extend(labels)\n",
    "            eval_preds.extend(predictions)\n",
    "            eval_scores.extend(flattened_scores)\n",
    "            \n",
    "            tmp_eval_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    labels = [id.item() for id in eval_labels]\n",
    "    predictions = [id.item() for id in eval_preds]\n",
    "    scores = [id.item() for id in eval_scores]\n",
    "    \n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    eval_accuracy = eval_accuracy / nb_eval_steps\n",
    "    print(f\"Validation Loss: {eval_loss}\")\n",
    "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
    "\n",
    "    return labels, predictions, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'DNATokenizer'.\n",
      "Some weights of the model checkpoint at 6-new-12w-0/ were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at 6-new-12w-0/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 0 epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss per 1000 training steps: 0.7216289639472961\n",
      "Training loss per 1000 training steps: 0.2036324538054181\n",
      "Training loss epoch: 0.13481990928590937\n",
      "Training accuracy epoch: 0.9372454119158233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss per 100 evaluation steps: 0.018579328432679176\n",
      "Validation loss per 100 evaluation steps: 0.03104487048821653\n",
      "Validation loss per 100 evaluation steps: 0.030050874836580706\n",
      "Validation loss per 100 evaluation steps: 0.030719055785283916\n",
      "Validation loss per 100 evaluation steps: 0.030411843780922862\n",
      "Validation loss per 100 evaluation steps: 0.03010581801050601\n",
      "Validation loss per 100 evaluation steps: 0.03042517472934071\n",
      "Validation Loss: 0.0303164715816723\n",
      "Validation Accuracy: 0.9848777361070358\n",
      "Fold 0 validation ROC-AUC:  0.9930483414919432\n",
      "Training fold 0 epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss per 1000 training steps: 0.04905088245868683\n",
      "Training loss per 1000 training steps: 0.030842333451493995\n",
      "Training loss epoch: 0.029580841195516804\n",
      "Training accuracy epoch: 0.9854959621646102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss per 100 evaluation steps: 0.016992032527923584\n",
      "Validation loss per 100 evaluation steps: 0.0264325980419791\n",
      "Validation loss per 100 evaluation steps: 0.0258730150149922\n",
      "Validation loss per 100 evaluation steps: 0.026456306944523885\n",
      "Validation loss per 100 evaluation steps: 0.026102901547868502\n",
      "Validation loss per 100 evaluation steps: 0.02578748909625487\n",
      "Validation loss per 100 evaluation steps: 0.026172021450203673\n",
      "Validation Loss: 0.02603699773872863\n",
      "Validation Accuracy: 0.987382565430197\n",
      "Fold 0 validation ROC-AUC:  0.9950974093319509\n",
      "Training fold 0 epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss per 1000 training steps: 0.04006458818912506\n",
      "Training loss per 1000 training steps: 0.02695930921835462\n",
      "Training loss epoch: 0.026491967630003537\n",
      "Training accuracy epoch: 0.9872866853563813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss per 100 evaluation steps: 0.01658421754837036\n",
      "Validation loss per 100 evaluation steps: 0.026167614127297213\n",
      "Validation loss per 100 evaluation steps: 0.025434170491108438\n",
      "Validation loss per 100 evaluation steps: 0.025869674179656587\n",
      "Validation loss per 100 evaluation steps: 0.025480109907656517\n",
      "Validation loss per 100 evaluation steps: 0.025181281276139247\n",
      "Validation loss per 100 evaluation steps: 0.025634712732056877\n",
      "Validation Loss: 0.025489698444272308\n",
      "Validation Accuracy: 0.9879193691518323\n",
      "Fold 0 validation ROC-AUC:  0.9954306836451036\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99   5108814\n",
      "           1       0.71      0.79      0.75    119218\n",
      "\n",
      "    accuracy                           0.99   5228032\n",
      "   macro avg       0.85      0.89      0.87   5228032\n",
      "weighted avg       0.99      0.99      0.99   5228032\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at 6-new-12w-0/ were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at 6-new-12w-0/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1 epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss per 1000 training steps: 0.6260885000228882\n",
      "Training loss per 1000 training steps: 0.18477940108891314\n",
      "Training loss epoch: 0.1245034057715389\n",
      "Training accuracy epoch: 0.9489285949902483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss per 100 evaluation steps: 0.03824375942349434\n",
      "Validation loss per 100 evaluation steps: 0.03207040742722036\n",
      "Validation loss per 100 evaluation steps: 0.0316234172405258\n",
      "Validation loss per 100 evaluation steps: 0.03151304540734899\n",
      "Validation loss per 100 evaluation steps: 0.031683331678969556\n",
      "Validation loss per 100 evaluation steps: 0.0322721942469137\n",
      "Validation loss per 100 evaluation steps: 0.03270215430372405\n",
      "Validation Loss: 0.032309378916028766\n",
      "Validation Accuracy: 0.983638948491295\n",
      "Fold 1 validation ROC-AUC:  0.9929448063491918\n",
      "Training fold 1 epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss per 1000 training steps: 0.007341303396970034\n",
      "Training loss per 1000 training steps: 0.029751540259360432\n",
      "Training loss epoch: 0.029322197569285755\n",
      "Training accuracy epoch: 0.9856985182738097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss per 100 evaluation steps: 0.03384003788232803\n",
      "Validation loss per 100 evaluation steps: 0.026698009576648474\n",
      "Validation loss per 100 evaluation steps: 0.026610505068674684\n",
      "Validation loss per 100 evaluation steps: 0.02671930894017393\n",
      "Validation loss per 100 evaluation steps: 0.02696527031698758\n",
      "Validation loss per 100 evaluation steps: 0.027473453962972676\n",
      "Validation loss per 100 evaluation steps: 0.027844528826404445\n",
      "Validation Loss: 0.027435982345833596\n",
      "Validation Accuracy: 0.9864026280076291\n",
      "Fold 1 validation ROC-AUC:  0.9949072043130884\n",
      "Training fold 1 epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss per 1000 training steps: 0.005427472293376923\n",
      "Training loss per 1000 training steps: 0.025931890837366245\n",
      "Training loss epoch: 0.026263614069423107\n",
      "Training accuracy epoch: 0.9875627196314943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss per 100 evaluation steps: 0.03339527174830437\n",
      "Validation loss per 100 evaluation steps: 0.026231387404190138\n",
      "Validation loss per 100 evaluation steps: 0.026204176369906212\n",
      "Validation loss per 100 evaluation steps: 0.026352065519439024\n",
      "Validation loss per 100 evaluation steps: 0.026571634201378796\n",
      "Validation loss per 100 evaluation steps: 0.026984436559421614\n",
      "Validation loss per 100 evaluation steps: 0.02736377251115023\n",
      "Validation Loss: 0.026957406415135842\n",
      "Validation Accuracy: 0.9867839305800078\n",
      "Fold 1 validation ROC-AUC:  0.9950788646235972\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99   5104927\n",
      "           1       0.69      0.79      0.74    122593\n",
      "\n",
      "    accuracy                           0.99   5227520\n",
      "   macro avg       0.84      0.89      0.87   5227520\n",
      "weighted avg       0.99      0.99      0.99   5227520\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at 6-new-12w-0/ were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at 6-new-12w-0/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 2 epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss per 1000 training steps: 0.7429850697517395\n",
      "Training loss per 1000 training steps: 0.21467667008163063\n",
      "Training loss epoch: 0.14115437869716888\n",
      "Training accuracy epoch: 0.9318122676554819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss per 100 evaluation steps: 0.029838915914297104\n",
      "Validation loss per 100 evaluation steps: 0.033168299344949206\n",
      "Validation loss per 100 evaluation steps: 0.031982939847888636\n",
      "Validation loss per 100 evaluation steps: 0.031293435726141516\n",
      "Validation loss per 100 evaluation steps: 0.03181512046029369\n",
      "Validation loss per 100 evaluation steps: 0.031460478617447225\n",
      "Validation loss per 100 evaluation steps: 0.03165477177844121\n",
      "Validation Loss: 0.03158288175992213\n",
      "Validation Accuracy: 0.9842536938209115\n",
      "Fold 2 validation ROC-AUC:  0.9926504383083921\n",
      "Training fold 2 epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss per 1000 training steps: 0.02691478095948696\n",
      "Training loss per 1000 training steps: 0.03093703778518306\n",
      "Training loss epoch: 0.02969762747073119\n",
      "Training accuracy epoch: 0.9852982240652562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss per 100 evaluation steps: 0.028643380850553513\n",
      "Validation loss per 100 evaluation steps: 0.028450312097510783\n",
      "Validation loss per 100 evaluation steps: 0.027425564371457146\n",
      "Validation loss per 100 evaluation steps: 0.026803434942462217\n",
      "Validation loss per 100 evaluation steps: 0.027204178001945304\n",
      "Validation loss per 100 evaluation steps: 0.026848661245781922\n",
      "Validation loss per 100 evaluation steps: 0.027035880778077837\n",
      "Validation Loss: 0.02698803013037593\n",
      "Validation Accuracy: 0.9869487923635564\n",
      "Fold 2 validation ROC-AUC:  0.9947212370092419\n",
      "Training fold 2 epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss per 1000 training steps: 0.022322745993733406\n",
      "Training loss per 1000 training steps: 0.027090102716067155\n",
      "Training loss epoch: 0.026576199506564868\n",
      "Training accuracy epoch: 0.9872553992311522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss per 100 evaluation steps: 0.029554123058915138\n",
      "Validation loss per 100 evaluation steps: 0.027957343685413055\n",
      "Validation loss per 100 evaluation steps: 0.02679984650310518\n",
      "Validation loss per 100 evaluation steps: 0.026269773504792704\n",
      "Validation loss per 100 evaluation steps: 0.026669120202955796\n",
      "Validation loss per 100 evaluation steps: 0.026361561178168406\n",
      "Validation loss per 100 evaluation steps: 0.02653848334446475\n",
      "Validation Loss: 0.026498525838494442\n",
      "Validation Accuracy: 0.9870867184443466\n",
      "Fold 2 validation ROC-AUC:  0.9950513252089095\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99   5108010\n",
      "           1       0.69      0.78      0.73    119510\n",
      "\n",
      "    accuracy                           0.99   5227520\n",
      "   macro avg       0.84      0.88      0.86   5227520\n",
      "weighted avg       0.99      0.99      0.99   5227520\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at 6-new-12w-0/ were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at 6-new-12w-0/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 3 epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss per 1000 training steps: 0.6384935975074768\n",
      "Training loss per 1000 training steps: 0.18530515620724988\n",
      "Training loss epoch: 0.12423004947857741\n",
      "Training accuracy epoch: 0.9494374561457037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss per 100 evaluation steps: 0.014740596525371075\n",
      "Validation loss per 100 evaluation steps: 0.03173416807422285\n",
      "Validation loss per 100 evaluation steps: 0.03046214840993452\n",
      "Validation loss per 100 evaluation steps: 0.030639020977866683\n",
      "Validation loss per 100 evaluation steps: 0.03140371942287659\n",
      "Validation loss per 100 evaluation steps: 0.030954823279223533\n",
      "Validation loss per 100 evaluation steps: 0.030726015245651953\n",
      "Validation Loss: 0.03046789959365289\n",
      "Validation Accuracy: 0.9850193554993153\n",
      "Fold 3 validation ROC-AUC:  0.9930440799513562\n",
      "Training fold 3 epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss per 1000 training steps: 0.03714064136147499\n",
      "Training loss per 1000 training steps: 0.03108356455173511\n",
      "Training loss epoch: 0.02982534512797574\n",
      "Training accuracy epoch: 0.9851030257574348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss per 100 evaluation steps: 0.014775535091757774\n",
      "Validation loss per 100 evaluation steps: 0.027430511639328987\n",
      "Validation loss per 100 evaluation steps: 0.02655272729624183\n",
      "Validation loss per 100 evaluation steps: 0.02668833486373038\n",
      "Validation loss per 100 evaluation steps: 0.027292054805392516\n",
      "Validation loss per 100 evaluation steps: 0.02700201277897606\n",
      "Validation loss per 100 evaluation steps: 0.026886311606771334\n",
      "Validation Loss: 0.026646282903450003\n",
      "Validation Accuracy: 0.9870290263717723\n",
      "Fold 3 validation ROC-AUC:  0.9950668736326906\n",
      "Training fold 3 epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss per 1000 training steps: 0.029402175918221474\n",
      "Training loss per 1000 training steps: 0.027285864839067944\n",
      "Training loss epoch: 0.026807808983998788\n",
      "Training accuracy epoch: 0.9870178373131172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss per 100 evaluation steps: 0.01326438132673502\n",
      "Validation loss per 100 evaluation steps: 0.027037754440495727\n",
      "Validation loss per 100 evaluation steps: 0.026325445162336255\n",
      "Validation loss per 100 evaluation steps: 0.026374890179716767\n",
      "Validation loss per 100 evaluation steps: 0.026815439856604225\n",
      "Validation loss per 100 evaluation steps: 0.026577105749855764\n",
      "Validation loss per 100 evaluation steps: 0.02652701882129464\n",
      "Validation Loss: 0.026288286217667618\n",
      "Validation Accuracy: 0.9872920792987089\n",
      "Fold 3 validation ROC-AUC:  0.9952337224515944\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99   5108607\n",
      "           1       0.69      0.81      0.74    118913\n",
      "\n",
      "    accuracy                           0.99   5227520\n",
      "   macro avg       0.84      0.90      0.87   5227520\n",
      "weighted avg       0.99      0.99      0.99   5227520\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at 6-new-12w-0/ were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at 6-new-12w-0/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 4 epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss per 1000 training steps: 0.5915674567222595\n",
      "Training loss per 1000 training steps: 0.163775293325836\n",
      "Training loss epoch: 0.11183312744368415\n",
      "Training accuracy epoch: 0.9565023742747506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss per 100 evaluation steps: 0.04031173139810562\n",
      "Validation loss per 100 evaluation steps: 0.02923372572306359\n",
      "Validation loss per 100 evaluation steps: 0.03156999440224313\n",
      "Validation loss per 100 evaluation steps: 0.031782953229506944\n",
      "Validation loss per 100 evaluation steps: 0.03159628671470872\n",
      "Validation loss per 100 evaluation steps: 0.03157001194690992\n",
      "Validation loss per 100 evaluation steps: 0.03174624307632769\n",
      "Validation Loss: 0.03205995988144491\n",
      "Validation Accuracy: 0.9836628276604069\n",
      "Fold 4 validation ROC-AUC:  0.9926550498146115\n",
      "Training fold 4 epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss per 1000 training steps: 0.03604988753795624\n",
      "Training loss per 1000 training steps: 0.03043646531316315\n",
      "Training loss epoch: 0.029645176529287897\n",
      "Training accuracy epoch: 0.9853287775042265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss per 100 evaluation steps: 0.04173064976930618\n",
      "Validation loss per 100 evaluation steps: 0.024915910842321295\n",
      "Validation loss per 100 evaluation steps: 0.02741922919212178\n",
      "Validation loss per 100 evaluation steps: 0.027873950557894635\n",
      "Validation loss per 100 evaluation steps: 0.027659079785159142\n",
      "Validation loss per 100 evaluation steps: 0.027502683468140707\n",
      "Validation loss per 100 evaluation steps: 0.02765760065665385\n",
      "Validation Loss: 0.027903418464029387\n",
      "Validation Accuracy: 0.986402819040982\n",
      "Fold 4 validation ROC-AUC:  0.9946813208252935\n",
      "Training fold 4 epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss per 1000 training steps: 0.03428428992629051\n",
      "Training loss per 1000 training steps: 0.026661128611662067\n",
      "Training loss epoch: 0.026647961104084048\n",
      "Training accuracy epoch: 0.9871686722413162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss per 100 evaluation steps: 0.04064717888832092\n",
      "Validation loss per 100 evaluation steps: 0.024351452265192967\n",
      "Validation loss per 100 evaluation steps: 0.026515456378710136\n",
      "Validation loss per 100 evaluation steps: 0.02712964755328267\n",
      "Validation loss per 100 evaluation steps: 0.026808861492694055\n",
      "Validation loss per 100 evaluation steps: 0.026675443761719916\n",
      "Validation loss per 100 evaluation steps: 0.02684886656320358\n",
      "Validation Loss: 0.027058914514383848\n",
      "Validation Accuracy: 0.9869845156005478\n",
      "Fold 4 validation ROC-AUC:  0.994852651622968\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99   5107137\n",
      "           1       0.71      0.74      0.72    120383\n",
      "\n",
      "    accuracy                           0.99   5227520\n",
      "   macro avg       0.85      0.87      0.86   5227520\n",
      "weighted avg       0.99      0.99      0.99   5227520\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = 1\n",
    "lr = 1e-5\n",
    "EPOCHS = 3\n",
    "\n",
    "dir_to_pretrained_model = \"6-new-12w-0/\"\n",
    "config = BertConfig.from_pretrained('https://raw.githubusercontent.com/jerryji1993/DNABERT/master/src/transformers/dnabert-config/bert-config-6/config.json')\n",
    "tokenizer = DNATokenizer.from_pretrained('6-new-12w-0/')\n",
    "\n",
    "for MODEL_NUMBER in range(5):\n",
    "    train_dataset, test_dataset = load(f'ds_w_seq_mm_fold{MODEL_NUMBER}_kouzine.pkl')\n",
    "    training_loader = DataLoader(train_dataset, batch_size=24, num_workers = 2)\n",
    "    testing_loader = DataLoader(test_dataset, batch_size=16, num_workers = 2)\n",
    "\n",
    "\n",
    "    model = BertForTokenClassification.from_pretrained(dir_to_pretrained_model, config=config)\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, steps_per_epoch=len(training_loader), epochs=EPOCHS)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"Training fold {MODEL_NUMBER} epoch: {epoch + 1}\")\n",
    "        train(epoch)\n",
    "        labels, predictions, scores = valid(model, testing_loader)\n",
    "        print(f'Fold {MODEL_NUMBER} validation ROC-AUC: ', roc_auc_score(labels, scores))\n",
    "\n",
    "    print(sklearn.metrics.classification_report(labels, predictions))\n",
    "    model.save_pretrained(f'dnabert_mm_fold_{MODEL_NUMBER}_kouzine')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
