{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dumerenkov/anaconda3/lib/python3.7/site-packages/pandas/compat/_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/home/dumerenkov/anaconda3/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "import random\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from tqdm import trange\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import os\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from joblib import Parallel, delayed, dump, load\n",
    "from matplotlib import pyplot as plt\n",
    "from sparse_vector.sparse_vector import SparseVector\n",
    "from scipy.signal import convolve2d, convolve\n",
    "import time\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torch\n",
    "from transformers import BertModel, BertConfig, PreTrainedTokenizer, BasicTokenizer, BertForTokenClassification\n",
    "import collections\n",
    "\n",
    "from transformers import utils\n",
    "from bertviz import model_view, head_view\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "import gc\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dna_tokenizer import DNATokenizer, seq2kmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, chroms, features, \n",
    "                 dna_source, features_source, \n",
    "                 labels_source, intervals, tokenizer):\n",
    "\n",
    "        self.chroms = chroms\n",
    "        self.features = features\n",
    "        self.dna_source = dna_source\n",
    "        self.features_source = features_source\n",
    "        self.labels_source = labels_source\n",
    "        self.intervals = intervals\n",
    "        self.le = LabelBinarizer().fit(np.array([[\"A\"], [\"C\"], [\"T\"], [\"G\"]]))\n",
    "        self.configs = {\n",
    "                        'ZHUNT_AS': {\n",
    "                                'CG': 0, 'GC': 1, 'CA': 0, 'AC': 1, \n",
    "                                'TG': 0, 'GT': 1, 'TA': 1, 'AT': 1, \n",
    "                                'CC': 0, 'GG': 0, 'CT': 1, 'TC': 1, \n",
    "                                'GA': 1, 'AG': 1, 'AA': 1, 'TT': 1},\n",
    "                       }\n",
    "        seqs = ([\"A\", \"C\", \"T\", \"G\"] + \n",
    "                ['AC', 'AT', 'AG', 'CT', 'CG', 'GT'] +\n",
    "                ['AAC', 'ACC', 'AAT', 'ATT', 'AAG', 'AGG', \n",
    "                 'CCA', 'CAA', 'CCT', 'CTT', 'CCG', 'CGG', \n",
    "                 'TTA', 'TAA', 'TTC', 'TCC', 'TTG', 'TGG', \n",
    "                 'GGA', 'GAA', 'GGC', 'GCC', 'GGT', 'GTT'] +\n",
    "                ['AAAC', 'AAAT', 'AAAG', 'CCCA', 'CCCT', 'CCCG',\n",
    "                 'TTTA', 'TTTC', 'TTTG', 'GGGA', 'GGGC', 'GGGT'])\n",
    "        self.tars = np.array([self.le.transform(list(i * 11)[:11]) for i in seqs])[:, ::-1, ::-1]\n",
    "        # purine-pyrimidine\n",
    "        self.tars = np.concatenate((self.tars, np.array([self.tars[4] + self.tars[9]])))\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.intervals)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        interval = self.intervals[index]\n",
    "        chrom = interval[0]\n",
    "        begin = int(interval[1])\n",
    "        end = int(interval[2])\n",
    "        ll = list(self.dna_source[chrom][begin:end].upper())\n",
    "        y = self.labels_source[interval[0]][interval[1]: interval[2]]        \n",
    "        \n",
    "        \n",
    "#         DNA PART\n",
    "        \n",
    "        dna_OHE = self.le.transform(ll)[None]\n",
    "        \n",
    "        res = pd.DataFrame(convolve(dna_OHE, self.tars)[:, 5:-5, 3].T / 11)\n",
    "        res = (res.rolling(5, min_periods=1).max().values == 1).astype(int)\n",
    "        \n",
    "        \n",
    "#         ZHUNT PART\n",
    "        zhunts = []\n",
    "        for key in self.configs:\n",
    "            vec = np.array(ll)\n",
    "            vec = np.vectorize(lambda x:self.configs[key].get(x, 0))(\n",
    "                                    np.char.add(vec[1:], vec[:-1]))\n",
    "            zhunts.append(np.concatenate([vec, [0]]))\n",
    "        \n",
    "        \n",
    "        # FEATURES PART\n",
    "        feature_matr = []\n",
    "        for feature in self.features:\n",
    "            source = self.features_source[feature]\n",
    "            feature_matr.append(source[chrom][begin:end])\n",
    "        \n",
    "        # UNION\n",
    "        if len(feature_matr) > 0:\n",
    "            X = np.hstack((\n",
    "                           res,\n",
    "                           np.array(zhunts).T, \n",
    "                           np.array(feature_matr).T/1000)).astype(np.float32)\n",
    "#             X = (np.array(feature_matr).T/1000).astype(np.float32)\n",
    "        else:\n",
    "            X = dna_OHE.astype(np.float32)\n",
    "        \n",
    "        #K-mer part\n",
    "        \n",
    "        k_mers = seq2kmer(self.dna_source[chrom][begin:end+5].upper(),6)\n",
    "        encoded_k_mers = self.tokenizer.encode_plus(k_mers, add_special_tokens=False, max_length=512)[\"input_ids\"]\n",
    "\n",
    "        return torch.Tensor(X), torch.Tensor(y).long(), ll, torch.LongTensor(encoded_k_mers), (chrom, begin, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 512\n",
    "k_mer_pad = 5\n",
    "\n",
    "def final_prediction(chrom):\n",
    "    \n",
    "    intervals = []\n",
    "    ends = []\n",
    "    \n",
    "    prediction = [np.zeros(ZDNA[chrom].shape, dtype=np.float32) for i in range(5)]\n",
    "    \n",
    "    \n",
    "    for st in range(0, ZDNA[chrom].shape - width, width):\n",
    "        interval = [st, min(st + width + k_mer_pad, ZDNA[chrom].shape)]\n",
    "        intervals.append([chrom, interval[0], interval[1]])\n",
    "        \n",
    "    ends.append([chrom, ZDNA[chrom].shape // width * width, ZDNA[chrom].shape])\n",
    "\n",
    "\n",
    "    full_dataset = Dataset(chroms, [], \n",
    "                           DNA, [],  ZDNA, intervals, tokenizer)\n",
    "\n",
    "    ends_dataset = Dataset(chroms, [], \n",
    "                           DNA, [], ZDNA, ends, tokenizer)\n",
    "\n",
    "    params = {'batch_size':32, 'num_workers':5, 'shuffle':False}\n",
    "    load_predict = data.DataLoader(full_dataset, **params)\n",
    "\n",
    "    params = {'batch_size':1, 'num_workers':1, 'shuffle':False}\n",
    "    load_ends = data.DataLoader(ends_dataset, **params)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for features, labels, sequences, input_ids, intervals in load_predict:\n",
    "            input_ids = input_ids.to(device)\n",
    "            outputs = [torch.softmax(model(input_ids = input_ids)['logits'],axis = -1).cpu().numpy()[:,:,1] for model in models]\n",
    "            \n",
    "            for ind, interval in enumerate(zip(intervals[0], \n",
    "                                         intervals[1].cpu().numpy(),\n",
    "                                         intervals[2].cpu().numpy())):\n",
    "                #print(ind, interval)\n",
    "                #print(len(prediction), len(outputs), len(models))\n",
    "                for i in range(len(models)):\n",
    "                    prediction[i][interval[1]:interval[2]-k_mer_pad] = outputs[i][ind, :]\n",
    "                    \n",
    "    for model_i in range(len(models)):\n",
    "        dump(prediction[model_i], f'new_mod_mm_{model_i}_{chrom}_curax', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 2\n",
    "torch.cuda.empty_cache()\n",
    "models = []\n",
    "\n",
    "config = BertConfig.from_pretrained('https://raw.githubusercontent.com/jerryji1993/DNABERT/master/src/transformers/dnabert-config/bert-config-6/config.json')\n",
    "\n",
    "for MODEL_NUMBER in range(5):\n",
    "    dir_to_pretrained_model = f'dnabert_mm_fold_{MODEL_NUMBER}_chipseq'\n",
    "    model = BertForTokenClassification.from_pretrained(dir_to_pretrained_model, config=config).to(device)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [00:03<00:00,  5.33it/s]\n"
     ]
    }
   ],
   "source": [
    "chroms = [f'chr{i}' for i in list(range(1, 20)) + ['X', 'Y']]\n",
    "DNA = {chrom:load(f'../data/mm9_dna/sparse/{chrom}.pkl') for chrom in tqdm(chroms)}\n",
    "ZDNA = load(f'../data/mm9_zdna/sparse/curax_14h_UNI_mm9.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'DNATokenizer'.\n"
     ]
    }
   ],
   "source": [
    "from dna_tokenizer import DNATokenizer, seq2kmer\n",
    "tokenizer = DNATokenizer.from_pretrained('6-new-12w-0/', add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN CHROM chr1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN CHROM chr2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN CHROM chr3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN CHROM chr4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN CHROM chr5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN CHROM chr6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN CHROM chr7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN CHROM chr8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN CHROM chr9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN CHROM chr10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN CHROM chr11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN CHROM chr12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN CHROM chr13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN CHROM chr14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN CHROM chr15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN CHROM chr16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN CHROM chr17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN CHROM chr18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN CHROM chr19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN CHROM chrX\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN CHROM chrY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "for chrom in chroms[:]:\n",
    "    print(f\"BEGIN CHROM {chrom}\")\n",
    "    final_prediction(chrom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "equalized, divisions = load('mm_divisions_chipseq.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [06:08<00:00, 17.56s/it]\n"
     ]
    }
   ],
   "source": [
    "com_len = sum([len(DNA[chrom]) for chrom in chroms])\n",
    "sums = []\n",
    "\n",
    "for chrom in tqdm(chroms):\n",
    "    loc_sum = []\n",
    "    for model_num in range(5):\n",
    "        vec = load(f\"preds/new_mod_mm_{model_num}_{chrom}_curax\")\n",
    "        loc_sum.append(vec.sum())\n",
    "    sums.append(loc_sum)\n",
    "\n",
    "multipliers = np.array(sums).sum(axis=0) / com_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [21:42<00:00, 62.04s/it]\n"
     ]
    }
   ],
   "source": [
    "for chrom in tqdm(chroms):\n",
    "    vecs = np.array([load(f\"preds/new_mod_mm_{model_num}_{chrom}_curax\") \n",
    "                     for model_num in range(5)])\n",
    "    res_vec = (vecs / multipliers[:, None]) * multipliers.mean()\n",
    "    mean_vec = res_vec.mean(axis=0)\n",
    "    \n",
    "    test_ints = []\n",
    "    for MODEL_NUMBER in range(5):\n",
    "        train_inds, test_inds = divisions[MODEL_NUMBER]\n",
    "        train_intervals, test_intervals = [equalized[i] for i in train_inds], [equalized[i] for i in test_inds]\n",
    "        test_ints.extend([(MODEL_NUMBER, inter) for inter in test_intervals if inter[0] == chrom])\n",
    "    \n",
    "    for model_num, inters in test_ints:\n",
    "        mean_vec[inters[1]: inters[2]] = res_vec[model_num, inters[1]: inters[2]]\n",
    "    dump(mean_vec, f\"preds/new_mod_mm_res_{chrom}_curax\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "blacklist = {c:[] for c in chroms}\n",
    "with open(\"mm9-blacklist.bed\")as f:\n",
    "    for line in f:\n",
    "        chrom, start, end =  line.strip().split()\n",
    "        blacklist[chrom].append((int(start), int(end)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [15:19<00:00, 43.77s/it]\n"
     ]
    }
   ],
   "source": [
    "for chrom in tqdm(chroms):\n",
    "    vec = load(f\"preds/new_mod_mm_res_{chrom}_curax\")\n",
    "    for s_idx, e_idx in blacklist[chrom]:\n",
    "        vec[s_idx:e_idx] = 0\n",
    "    dump(vec, f\"preds/new_mod_mm_res_{chrom}_curax\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [02:38<00:00,  7.56s/it]\n"
     ]
    }
   ],
   "source": [
    "all_pred = []\n",
    "all_true = []\n",
    "for chrom in tqdm(chroms):\n",
    "    all_pred.append(load(f\"preds/new_mod_mm_res_{chrom}_curax\"))\n",
    "    all_true.append(ZDNA[chrom][:].astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8362092912637998"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(np.concatenate(all_true), np.concatenate(all_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9998    0.9970    0.9984 2654081885\n",
      "           1     0.0365    0.3707    0.0664    813333\n",
      "\n",
      "    accuracy                         0.9968 2654895218\n",
      "   macro avg     0.5181    0.6839    0.5324 2654895218\n",
      "weighted avg     0.9995    0.9968    0.9981 2654895218\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.classification_report(np.concatenate(all_true), np.concatenate(all_pred)>0.5, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                         | 0/2790 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2790/2790 [01:38<00:00, 28.24it/s]\n",
      "558it [05:03,  1.84it/s]\n",
      "  0%|                                                                                                                         | 0/2790 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2790/2790 [01:38<00:00, 28.30it/s]\n",
      "558it [04:50,  1.92it/s]\n",
      "  0%|                                                                                                                         | 0/2790 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2790/2790 [01:38<00:00, 28.25it/s]\n",
      "558it [04:57,  1.87it/s]\n",
      "  0%|                                                                                                                         | 0/2790 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2790/2790 [01:35<00:00, 29.29it/s]\n",
      "558it [04:42,  1.98it/s]\n",
      "  0%|                                                                                                                         | 0/2790 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2790/2790 [01:36<00:00, 28.93it/s]\n",
      "558it [05:06,  1.82it/s]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "kmer2pred = defaultdict(int)\n",
    "kmer2att = defaultdict(float)\n",
    "\n",
    "device = 3\n",
    "for FOLD in range(5):\n",
    "    gc.collect()\n",
    "    attentions, preds, targets, seqs, bps = [],[],[], [], []\n",
    "    train_dataset, test_dataset = load(f'/gim/lv01/dumerenkov/zdna_data/datasets/ds_w_seq_mm_fold{FOLD}_chipseq.pkl')\n",
    "    model = BertForTokenClassification.from_pretrained(f'dnabert_mm_fold_{FOLD}_chipseq', output_attentions=True)\n",
    "    model.to(device)\n",
    "    for example in tqdm(test_dataset):\n",
    "        features, target, seq, input_ids, interval = example\n",
    "        if target.numpy().sum()>0:\n",
    "            with torch.no_grad():\n",
    "                input_ids = input_ids.to(device)\n",
    "                outputs = model(input_ids.unsqueeze(0))\n",
    "                \n",
    "                raw_preds = outputs[-2].detach().to('cpu')\n",
    "                attention = outputs[-1][-1].detach().to('cpu')\n",
    "                \n",
    "                pred = torch.softmax(raw_preds, axis = -1)[0,:,1]\n",
    "\n",
    "                \n",
    "            attentions.append(attention)\n",
    "            preds.append(pred)\n",
    "            targets.append(target)\n",
    "            seqs.append(seq)\n",
    "            \n",
    "    for attention, pred, target, seq in tqdm(zip(attentions, preds, targets, seqs)):\n",
    "        kmer = seq2kmer(''.join(seq), 6).split(' ')\n",
    "        #print(kmer)\n",
    "        att = attention[0,:,:,:]\n",
    "    \n",
    "        for idx in range(512-5):\n",
    "            if target[idx]>0:\n",
    "                kmer2pred[kmer[idx]]+=1\n",
    "            \n",
    "                for head in range(12):\n",
    "                    c_att = att[head,idx,:].numpy()\n",
    "                    for att_idx in range(512-5):\n",
    "                        kmer2att[kmer[att_idx]]+=c_att[att_idx]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_pred = [t[0] for t in sorted(kmer2pred.items(), key=lambda item: -item[1])]\n",
    "kmer2att2 = {k:int(kmer2att[k]) for k in kmer2att}\n",
    "sorted_att  = [t[0] for t in sorted(kmer2att2.items(), key=lambda item: -item[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 AAGAAG 1\n",
      "2 AGAAGA 2\n",
      "3 GAAGAA 3\n",
      "4 CTTCTT 4\n",
      "5 CTCGAG 13\n",
      "6 TCTTCT 5\n",
      "7 CACACA 7\n",
      "8 CAGCAG 6\n",
      "9 TCCTCC 9\n",
      "10 TTCTTC 8\n",
      "11 GGAGGA 12\n",
      "12 CCCGGG 11\n",
      "13 AAAAAA 14\n",
      "14 CTGCTG 10\n",
      "15 CTGCCT 19\n",
      "16 CAGAGG 16\n",
      "17 GTCCCG 18\n",
      "18 CTTAGG 21\n",
      "19 GCCGGC 25\n",
      "20 AGGAGG 29\n",
      "21 CTCTGG 23\n",
      "22 CCAGAG 17\n",
      "23 CGGGAC 27\n",
      "24 CCTCCT 34\n",
      "25 AGGCAG 42\n",
      "26 GAGGAG 24\n",
      "27 CGGAAG 71\n",
      "28 GCCCGG 30\n",
      "29 CTCCTC 32\n",
      "30 GGCCGG 48\n",
      "31 GGCGGG 61\n",
      "32 AGCAGC 15\n",
      "33 CCTAAG 33\n",
      "34 CCTCTG 20\n",
      "35 TGGGGA 70\n",
      "36 GGGCAC 134\n",
      "37 CAGCCG 60\n",
      "38 CTTCTG 38\n",
      "39 CAGGTG 36\n",
      "40 CAGAAG 53\n",
      "41 GACAGC 28\n",
      "42 CCTTCC 37\n",
      "43 CCGGCC 39\n",
      "44 TCCGGA 68\n",
      "45 TGGGGC 139\n",
      "46 GGAAGG 63\n",
      "47 GATTGG 117\n",
      "48 CGGCTG 97\n",
      "49 GCAGCA 22\n",
      "50 ACACAC 26\n",
      "51 AACAGG 46\n",
      "52 GGGGAG 51\n",
      "53 TGCTGC 47\n",
      "54 CTGTCC 43\n",
      "55 GGACAG 40\n",
      "56 CCCGTG 52\n",
      "57 GCTGTC 35\n",
      "58 CACCTG 41\n",
      "59 GCAGGC 49\n",
      "60 ACAGGT 44\n",
      "61 GGGGCA 137\n",
      "62 CCGGGC 62\n",
      "63 GTGTGG 57\n",
      "64 CCGGAC 78\n",
      "65 CTGGGG 107\n",
      "66 CGGACC 114\n",
      "67 GCCGGG 82\n",
      "68 GCGGAA 91\n",
      "69 TCGAGT 128\n",
      "70 ACCAGA 104\n",
      "71 AGCCGG 72\n",
      "72 GTCCGG 95\n",
      "73 TGCCTC 94\n",
      "74 CCCGGC 79\n",
      "75 CCCGCC 73\n",
      "76 GGTGAG 50\n",
      "77 GCACCC 59\n",
      "78 CCGGGG 89\n",
      "79 AGGGTG 64\n",
      "80 GCTGCT 31\n",
      "81 CCCCAG 55\n",
      "82 CTGAGG 144\n",
      "83 CTTCCG 105\n",
      "84 GAGGCG 93\n",
      "85 CCACAC 90\n",
      "86 GAAGCA 58\n",
      "87 GGGACT 75\n",
      "88 AGAAGC 80\n",
      "89 TTTTTT 108\n",
      "90 CTCCCC 81\n",
      "91 TCCCGT 121\n",
      "92 CACGGG 84\n",
      "93 CAGGCA 122\n",
      "94 CTCTGC 96\n",
      "95 TGCTTC 45\n",
      "96 CACCCT 77\n",
      "97 GGACCC 74\n",
      "98 TGTGTG 101\n",
      "99 GAACAG 83\n",
      "100 ACCTGT 65\n",
      "101 TGAGAG 112\n",
      "102 GTGCCC 110\n"
     ]
    }
   ],
   "source": [
    "for idx, kmer in enumerate(sorted_att):\n",
    "    print(idx+1, kmer, sorted_pred.index(kmer)+1)\n",
    "    if idx>100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [02:39<00:00,  7.58s/it]\n"
     ]
    }
   ],
   "source": [
    "chroms = [f'chr{i}' for i in list(range(1, 20)) + ['X', 'Y']]\n",
    "ZDNA = load(f'../data/mm9_zdna/sparse/curax_14h_UNI_mm9.pkl')\n",
    "blacklist = {c:[] for c in chroms}\n",
    "with open(\"mm9-blacklist.bed\")as f:\n",
    "    for line in f:\n",
    "        chrom, start, end =  line.strip().split()\n",
    "        blacklist[chrom].append((int(start), int(end)))\n",
    "\n",
    "all_pred = []\n",
    "all_true = []\n",
    "for chrom in tqdm(chroms):\n",
    "    true_clean = ZDNA[chrom][:].astype(int)\n",
    "    for s_idx, e_idx in blacklist[chrom]:\n",
    "        true_clean[s_idx:e_idx] = 0\n",
    "    all_pred.append(load(f\"preds/new_mod_mm_res_{chrom}_curax\"))\n",
    "    all_true.append(true_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9580745042519483"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(np.concatenate(all_true), np.concatenate(all_pred))\n",
    "#0.5025639014990957"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9998    0.9970    0.9984 2654185510\n",
      "           1     0.0365    0.4249    0.0672    709708\n",
      "\n",
      "    accuracy                         0.9968 2654895218\n",
      "   macro avg     0.5182    0.7109    0.5328 2654895218\n",
      "weighted avg     0.9996    0.9968    0.9982 2654895218\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.classification_report(np.concatenate(all_true), np.concatenate(all_pred)>0.5, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
