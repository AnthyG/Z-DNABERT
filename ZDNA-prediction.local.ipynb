{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **[Jump to Run Section](#Run)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f59Ujuujn___",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Install dependencies and define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "apiUcTpNTnlU",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip3 install transformers\n",
    "!pip3 install biopython\n",
    "!pip3 install torch\n",
    "!pip3 install numpy\n",
    "!pip3 install scipy\n",
    "!pip3 install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bsyfz4BrSxMN"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "from io import StringIO, BytesIO\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import scipy\n",
    "from scipy import ndimage\n",
    "import os\n",
    "from ipywidgets import widgets\n",
    "import subprocess\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './6-new-12w-0'\n",
    "model_data_path = './pytorch_models'\n",
    "target_model_data_path = os.path.join(data_path, 'pytorch_model.bin')\n",
    "output_path = './output'\n",
    "file_name_maximum_length = 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5I6iDSnocLS3"
   },
   "outputs": [],
   "source": [
    "def seq2kmer(seq, k):\n",
    "    kmer = [seq[x:x+k] for x in range(len(seq)+1-k)]\n",
    "    return kmer\n",
    "\n",
    "def split_seq(seq, length = 512, pad = 16):\n",
    "    res = []\n",
    "    for st in range(0, len(seq), length - pad):\n",
    "        end = min(st+512, len(seq))\n",
    "        res.append(seq[st:end])\n",
    "    return res\n",
    "\n",
    "def stitch_np_seq(np_seqs, pad = 16):\n",
    "    res = np.array([])\n",
    "    for seq in np_seqs:\n",
    "        res = res[:-pad]\n",
    "        res = np.concatenate([res,seq])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_pair_opposite_map = {\n",
    "    'A': 'T',\n",
    "    'T': 'A',\n",
    "    'C': 'G',\n",
    "    'G': 'C',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complement_nucleobase(nucleobase):\n",
    "    return base_pair_opposite_map[nucleobase] if nucleobase in base_pair_opposite_map else nucleobase\n",
    "\n",
    "def complement_seq(seq):\n",
    "    return ''.join([complement_nucleobase(nucleobase) for nucleobase in seq])\n",
    "\n",
    "def reverse_seq(seq):\n",
    "    return seq[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seq = 'ACGTA'\n",
    "print('seq\\t\\t\\t', test_seq)\n",
    "print('complement\\t\\t', complement_seq(test_seq))\n",
    "print('reverse\\t\\t\\t', reverse_seq(test_seq))\n",
    "print('reverse-complement\\t', reverse_seq(complement_seq(test_seq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gdown_wrapper(gdrive_id, file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        print(file_path, gdrive_id, 'already exists')\n",
    "        return\n",
    "    \n",
    "    # curl \"https://drive.google.com/uc?id=${id}&export=download&confirm=ABCD\" --verbose -L -o \n",
    "    #gdrive_url = 'https://drive.google.com/uc?id={id}&export=download&confirm=ABCD'.format(id=gdrive_id)\n",
    "    gdrive_url = 'https://drive.usercontent.google.com/download?id={id}&confirm=ABCD'.format(id=gdrive_id)\n",
    "    \n",
    "    print(gdrive_url, file_path)\n",
    "    \n",
    "    !curl -L --progress-bar -o \"{file_path}\" \"{gdrive_url}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_file(file_path):\n",
    "    if os.path.exists(file_path) == False:\n",
    "        return None\n",
    "    \n",
    "    return subprocess.run(['shasum', file_path], stdout=subprocess.PIPE).stdout[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'HG kouzine': (\n",
    "        '1dAeAt5Gu2cadwDhbc7OnenUgDLHlUvkx',\n",
    "        'hg_kouzine.pytorch_model.bin',\n",
    "    ),\n",
    "    'HG chipseq': (\n",
    "        '1VAsp8I904y_J0PUhAQqpSlCn1IqfG0FB',\n",
    "        'hg_chipseq.pytorch_model.bin',\n",
    "    ),\n",
    "    'MM curax': (\n",
    "        '1W6GEgHNoitlB-xXJbLJ_jDW4BF35W1Sd',\n",
    "        'mm_curax.pytorch_model.bin',\n",
    "    ),\n",
    "    'MM kouzine': (\n",
    "        '1dXpQFmheClKXIEoqcZ7kgCwx6hzVCv3H',\n",
    "        'mm_kouzine.pytorch_model.bin',\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_files = [\n",
    "    ('10sF8Ywktd96HqAL0CwvlZZUUGj05CGk5', os.path.join(data_path, 'config.json')),\n",
    "    ('16bT7HDv71aRwyh3gBUbKwign1mtyLD2d', os.path.join(data_path, 'special_tokens_map.json')),\n",
    "    ('1EE9goZ2JRSD8UTx501q71lGCk-CK3kqG', os.path.join(data_path, 'tokenizer_config.json')),\n",
    "    ('1gZZdtAoDnDiLQqjQfGyuwt268Pe5sXW0', os.path.join(data_path, 'vocab.txt')),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xYq4WVAtcLS4"
   },
   "outputs": [],
   "source": [
    "model_name_widget = widgets.Dropdown(\n",
    "    options=models.keys(),\n",
    "    value=next(iter(models.keys())),\n",
    "    description='model:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "model_confidence_threshold_widget = widgets.FloatText(\n",
    "    value=0.5,\n",
    "    description='model confidence threshold'\n",
    ")\n",
    "\n",
    "minimum_sequence_length_widget = widgets.IntText(\n",
    "    value=10,\n",
    "    description='minimum sequence length:',\n",
    ")\n",
    "\n",
    "check_sequence_variations_widget = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description='check reverse complement sequence variations'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasta_upload_widget = widgets.FileUpload(\n",
    "    multiple=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model_output = widgets.Output()\n",
    "do_predictions_output = widgets.Output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = None\n",
    "model_confidence_threshold = None\n",
    "minimum_sequence_length = None\n",
    "check_sequence_variations = None\n",
    "model_file_path = None\n",
    "tokenizer = None\n",
    "model = None\n",
    "is_cuda_available = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@load_model_output.capture(clear_output=True)\n",
    "def load_model(btn):\n",
    "    global model_name, model_confidence_threshold, minimum_sequence_length, check_sequence_variations, model_file_path, tokenizer, model, is_cuda_available\n",
    "    \n",
    "    model_name = model_name_widget.value\n",
    "    model_confidence_threshold = model_confidence_threshold_widget.value\n",
    "    minimum_sequence_length = minimum_sequence_length_widget.value\n",
    "    check_sequence_variations = check_sequence_variations_widget.value\n",
    "    \n",
    "    model_gdrive_id, model_file_name = models[model_name]\n",
    "    \n",
    "    model_file_path = os.path.join(model_data_path, model_file_name)\n",
    "    \n",
    "    \n",
    "    print('downloading model data to input directory\\n')\n",
    "    \n",
    "    !mkdir \"{data_path}\"\n",
    "    !mkdir \"{model_data_path}\"\n",
    "    \n",
    "    gdown_wrapper(model_gdrive_id, model_file_path)\n",
    "    \n",
    "    for meta_file_gdrive_id, meta_file_file_path in meta_files:\n",
    "        gdown_wrapper(meta_file_gdrive_id, meta_file_file_path)\n",
    "    \n",
    "    \n",
    "    print('\\n\\nchecking model file in input directory\\n')\n",
    "    \n",
    "    hash1 = hash_file(model_file_path)\n",
    "    hash2 = hash_file(target_model_data_path)\n",
    "    print(hash1, hash2)\n",
    "    if hash1 != hash2:\n",
    "        print('\\ncopying model file to input directory\\n')\n",
    "        !cp \"{model_file_path}\" \"{target_model_data_path}\"\n",
    "    else:\n",
    "        print('\\nmodel hasn\\'t changed\\n')\n",
    "    \n",
    "    \n",
    "    print('\\n\\nloading model\\n')\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained(data_path)\n",
    "    model = BertForTokenClassification.from_pretrained(data_path)\n",
    "    is_cuda_available = torch.cuda.is_available()\n",
    "    print('cuda is', 'available' if is_cuda_available else 'not available')\n",
    "    if is_cuda_available:\n",
    "        model.cuda()\n",
    "    else:\n",
    "        model.cpu()\n",
    "    \n",
    "    print('\\n\\ncompleted loading model\\n\\nmodel: {}\\nmodel confidence threshold: {}\\nminimum sequence length: {}'.format(model_name, model_confidence_threshold, minimum_sequence_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model_button = widgets.Button(\n",
    "    description='Load model',\n",
    "    icon='truck-loading',\n",
    ")\n",
    "load_model_button.on_click(load_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_1qjq3i-VCEz"
   },
   "outputs": [],
   "source": [
    "@do_predictions_output.capture(clear_output=True)\n",
    "def do_predictions(btn):\n",
    "    uploaded = {v['name']: v['content'] for v in fasta_upload_widget.value}\n",
    "    for fn in uploaded.keys():\n",
    "        print('Fasta file \"{name}\" with length {length} bytes\\n'.format(name=fn, length=len(uploaded[fn])))\n",
    "    \n",
    "    out = []\n",
    "    \n",
    "    out.append('model_name: {}'.format(model_name))\n",
    "    print('model_name: {}'.format(model_name))\n",
    "    out.append('model_confidence: {}'.format(model_confidence_threshold))\n",
    "    print('model_confidence: {}'.format(model_confidence_threshold))\n",
    "    out.append('minimum_sequence_length: {}'.format(minimum_sequence_length))\n",
    "    print('minimum_sequence_length: {}'.format(minimum_sequence_length))\n",
    "    \n",
    "    model_params_as_string = 'm={},mct={},msl={}'.format(model_name, model_confidence_threshold, minimum_sequence_length)\n",
    "    model_params_as_string_for_file_name = 'm_{},mct_{},msl_{}'.format(model_name, model_confidence_threshold, minimum_sequence_length)\n",
    "    now_time_as_string_for_file_name = time.strftime(\"%Y_%m_%d,%H_%M_%S\")\n",
    "    \n",
    "    # <file name length checks>\n",
    "    file_names_to_check = []\n",
    "    \n",
    "    text_predictions_file_name = 'text_predictions.{}.{}.txt'.format(model_params_as_string_for_file_name, now_time_as_string_for_file_name)\n",
    "    file_names_to_check.append(text_predictions_file_name)\n",
    "    for key in uploaded.keys():\n",
    "        for seq_record in SeqIO.parse(StringIO(BytesIO(uploaded[key]).read().decode('UTF-8')), 'fasta'):\n",
    "            seqs = []\n",
    "            seqs.append('normal')\n",
    "            if check_sequence_variations:\n",
    "                seqs.append('reverse-complement')\n",
    "            \n",
    "            seq_record_key = '{}.{}.{}.{}'.format(key, seq_record.name, model_params_as_string_for_file_name, now_time_as_string_for_file_name)\n",
    "            \n",
    "            for seq_name in seqs:\n",
    "                seq_key = '{}.{}'.format(seq_record_key, seq_name)\n",
    "                \n",
    "                pkl_file_name_seq = '{}.pkl'.format(seq_key)\n",
    "                file_names_to_check.append(pkl_file_name_seq)\n",
    "                bed_file_name_seq = '{}.bed'.format(seq_key)\n",
    "                file_names_to_check.append(bed_file_name_seq)\n",
    "            bed_file_name = '{}.bed'.format(seq_record_key)\n",
    "            file_names_to_check.append(bed_file_name)\n",
    "    \n",
    "    file_names_too_long = [] # array of tuples: (file_name: String, file_name_length: Int)\n",
    "    for file_name_to_check in file_names_to_check:\n",
    "        file_name_to_check_length = len(file_name_to_check)\n",
    "        file_name_too_long = file_name_to_check_length > file_name_maximum_length\n",
    "        if file_name_too_long:\n",
    "            file_names_too_long.append((file_name_to_check, file_name_to_check_length))\n",
    "    \n",
    "    if len(file_names_too_long) > 0:\n",
    "        print('\\n\\nThe length of some file names that are going to be generated exceeds the maximum file name length of {} characters.\\nPlease reduce the lengths of the inputs accordingly.'.format(file_name_maximum_length))\n",
    "        for file_name, file_name_length in file_names_too_long:\n",
    "            print('\\nname:\\t{}\\nlength:\\t{}\\nover:\\t{}'.format(file_name, file_name_length, file_name_length - file_name_maximum_length))\n",
    "        raise ValueError('At least one of the generated file names will exceed the maximum file name length of {} characters.'.format(file_name_maximum_length))\n",
    "    # </file name length checks>\n",
    "    \n",
    "    \n",
    "    for key in uploaded.keys():\n",
    "        print(key)\n",
    "        out.append(key)\n",
    "        result_dict = {}\n",
    "        for seq_record in SeqIO.parse(StringIO(BytesIO(uploaded[key]).read().decode('UTF-8')), 'fasta'):\n",
    "            seqs = [] # array of tuples: (variation name: String, sequence: String, reversed: Bool)\n",
    "            seq_uppered = str(seq_record.seq).upper()\n",
    "            seqs.append(('normal', seq_uppered, False))\n",
    "            if check_sequence_variations:\n",
    "                seq_uppered_complemented = complement_seq(seq_uppered)\n",
    "                \n",
    "                seq_uppered_complemented_reversed = reverse_seq(seq_uppered_complemented)\n",
    "                seqs.append(('reverse-complement', seq_uppered_complemented_reversed, True))\n",
    "            \n",
    "            print(seq_record.name)\n",
    "            out.append(seq_record.name)\n",
    "            \n",
    "            seq_record_key = '{}.{}.{}.{}'.format(key, seq_record.name, model_params_as_string_for_file_name, now_time_as_string_for_file_name)\n",
    "            \n",
    "            bed_out = []\n",
    "            bed_out.append('track name=\"{name}\" priority=1'.format(name=model_params_as_string))\n",
    "            \n",
    "            for seq_name, seq, seq_reversed in seqs:\n",
    "                seq_key = '{}.{}'.format(seq_record_key, seq_name)\n",
    "                \n",
    "                seq_len = len(seq)\n",
    "                \n",
    "                bed_out_seq = []\n",
    "                \n",
    "                kmer_seq = seq2kmer(seq, 6)\n",
    "                seq_pieces = split_seq(kmer_seq)\n",
    "                print(seq_name)\n",
    "                out.append(seq_name)\n",
    "                with torch.no_grad():\n",
    "                    preds = []\n",
    "                    for seq_piece in tqdm(seq_pieces):\n",
    "                        input_ids = torch.LongTensor(tokenizer.encode(' '.join(seq_piece), add_special_tokens=False))\n",
    "                        input_ids_unsqueezed = None\n",
    "                        if is_cuda_available:\n",
    "                            input_ids_unsqueezed = input_ids.cuda().unsqueeze(0)\n",
    "                        else:\n",
    "                            input_ids_unsqueezed = input_ids.cpu().unsqueeze(0)\n",
    "                        outputs = torch.softmax(model(input_ids_unsqueezed)[-1],axis = -1)[0,:,1]\n",
    "                        preds.append(outputs.cpu().numpy())\n",
    "                result_dict[seq_key] = stitch_np_seq(preds)\n",
    "                \n",
    "                \n",
    "                \n",
    "                labeled, max_label = scipy.ndimage.label(result_dict[seq_key]>model_confidence_threshold)\n",
    "                print('  start     end')\n",
    "                out.append('  start     end')\n",
    "                \n",
    "                bed_out.append('track name=\"{name}\" priority=2'.format(name=seq_name))\n",
    "                \n",
    "                label_id = 1\n",
    "                for label in range(1, max_label+1):\n",
    "                    candidate = np.where(labeled == label)[0]\n",
    "                    candidate_length = candidate.shape[0]\n",
    "                    if candidate_length>minimum_sequence_length:\n",
    "                        print('{:8}'.format(candidate[0]), '{:8}'.format(candidate[-1]))\n",
    "                        out.append('{:8}{:8}'.format(candidate[0], candidate[-1]))\n",
    "                        \n",
    "                        # start has to be subtracted by 1 for bed, see https://grch37.ensembl.org/info/website/upload/bed.html\n",
    "                        candidate_start = candidate[0] - 1\n",
    "                        candidate_end = candidate[-1]\n",
    "                        if seq_reversed:\n",
    "                            candidate_start = (seq_len - candidate[-1]) - 1\n",
    "                            candidate_end = seq_len - candidate[0]\n",
    "                        \n",
    "                        bed_name = '{},{},{}'.format(model_params_as_string, seq_name, label_id)\n",
    "                        bed_out.append('0\\t{start}\\t{end}\\t{name}'.format(start=candidate_start, end=candidate_end, name=bed_name))\n",
    "                        \n",
    "                        bed_out_seq.append('0\\t{start}\\t{end}\\t{name}'.format(start=candidate_start, end=candidate_end, name=bed_name))\n",
    "                        \n",
    "                        label_id += 1\n",
    "\n",
    "                pkl_file_name_seq = '{}.pkl'.format(seq_key)\n",
    "                with open(os.path.join(output_path, pkl_file_name_seq),\"wb\") as fh:\n",
    "                  pickle.dump(result_dict, fh)\n",
    "                print()\n",
    "                \n",
    "                bed_file_name_seq = '{}.bed'.format(seq_key)\n",
    "                print(bed_file_name_seq)\n",
    "                with open(os.path.join(output_path, bed_file_name_seq),\"w\") as fh:\n",
    "                    for item in bed_out_seq:\n",
    "                        fh.write(\"%s\\n\" % item)\n",
    "            \n",
    "            bed_file_name = '{}.bed'.format(seq_record_key)\n",
    "            print(bed_file_name)\n",
    "            with open(os.path.join(output_path, bed_file_name),\"w\") as fh:\n",
    "                for item in bed_out:\n",
    "                    fh.write(\"%s\\n\" % item)\n",
    "    \n",
    "    with open(os.path.join(output_path, text_predictions_file_name),\"w\") as fh:\n",
    "        for item in out:\n",
    "            fh.write(\"%s\\n\" % item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_predictions_button = widgets.Button(\n",
    "    description='Run prediction',\n",
    "    icon='chart-line',\n",
    ")\n",
    "do_predictions_button.on_click(do_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir \"{output_path}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run\n",
    "\n",
    "Start predicting features of fasta file inputs in 4 steps.\n",
    "\n",
    "## Usage\n",
    "\n",
    "### Prepare\n",
    "\n",
    "Preparing the environment only needs to be done once everytime when starting JupyterLab or freshly opening the notebook thereafter.\n",
    "\n",
    "### Select model and parameters\n",
    "\n",
    "After changing the model or the parameters, press the \"Load model\"-Button.\n",
    "\n",
    "This will create required directories, download required files and move the model file into the relevant directory. Files that have been downloaded already, will not be downloaded again.\n",
    "\n",
    "### Run\n",
    "\n",
    "After the predictions have been made, new files will be created in the directory `output`.\n",
    "\n",
    "The following types of files will be created:\n",
    "\n",
    "- `.txt`-Files will contain the textual representation as seen in the output of the notebook for all input files\n",
    "- Several different `.bed`-Files containing the found features will be created for each input file based on the selected sequence variations\n",
    "\n",
    "  They can be used to import found features into other software.\n",
    "  \n",
    "  - `.normal.bed` contains features found for the original input fasta file\n",
    "  - `.reverse-complement.bed` contains features found for the reverse-complement\n",
    "  - `.bed` contains features found in both the normal and the reverse-complement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Prepare\n",
    "\n",
    "<button data-commandLinker-command=\"notebook:run-all-cells\" class=\"lm-Widget jupyter-widgets jupyter-button\">Prepare environment</button>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Select model and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(model_name_widget)\n",
    "display(model_confidence_threshold_widget)\n",
    "display(minimum_sequence_length_widget)\n",
    "display(check_sequence_variations_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(load_model_button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(load_model_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Upload fasta files\n",
    "\n",
    "Multiple fasta files may be selected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(fasta_upload_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(do_predictions_button)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "display(do_predictions_output)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "ZDNA prediction",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
