{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QeAG1NZ5cLSz"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mitiau/DNABERT-Z/blob/main/ZDNA-prediction.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f59Ujuujn___"
   },
   "source": [
    "# Install dependencies and define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "apiUcTpNTnlU",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip3 install transformers\n",
    "!pip3 install biopython\n",
    "!pip3 install torch\n",
    "!pip3 install numpy\n",
    "!pip3 install scipy\n",
    "!pip3 install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bsyfz4BrSxMN"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "from io import StringIO, BytesIO\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import scipy\n",
    "from scipy import ndimage\n",
    "import os\n",
    "from ipywidgets import widgets\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5I6iDSnocLS3"
   },
   "outputs": [],
   "source": [
    "def seq2kmer(seq, k):\n",
    "    kmer = [seq[x:x+k] for x in range(len(seq)+1-k)]\n",
    "    return kmer\n",
    "\n",
    "def split_seq(seq, length = 512, pad = 16):\n",
    "    res = []\n",
    "    for st in range(0, len(seq), length - pad):\n",
    "        end = min(st+512, len(seq))\n",
    "        res.append(seq[st:end])\n",
    "    return res\n",
    "\n",
    "def stitch_np_seq(np_seqs, pad = 16):\n",
    "    res = np.array([])\n",
    "    for seq in np_seqs:\n",
    "        res = res[:-pad]\n",
    "        res = np.concatenate([res,seq])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_pair_opposite_map = {\n",
    "    'A': 'T',\n",
    "    'T': 'A',\n",
    "    'C': 'G',\n",
    "    'G': 'C',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complement_nucleobase(nucleobase):\n",
    "    return base_pair_opposite_map[nucleobase] if nucleobase in base_pair_opposite_map else nucleobase\n",
    "\n",
    "def complement_seq(seq):\n",
    "    return ''.join([complement_nucleobase(nucleobase) for nucleobase in seq])\n",
    "\n",
    "def reverse_seq(seq):\n",
    "    return seq[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seq = 'ACGTA'\n",
    "print('seq\\t\\t\\t', test_seq)\n",
    "print('complement\\t\\t', complement_seq(test_seq))\n",
    "print('reverse\\t\\t\\t', reverse_seq(test_seq))\n",
    "print('reverse-complement\\t', reverse_seq(complement_seq(test_seq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gdown_wrapper(gdrive_id, file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        print(file_path, gdrive_id, 'already exists')\n",
    "        return\n",
    "    \n",
    "    # curl \"https://drive.google.com/uc?id=${id}&export=download&confirm=ABCD\" --verbose -L -o \n",
    "    gdrive_url = 'https://drive.google.com/uc?id={id}&export=download&confirm=ABCD'.format(id=gdrive_id)\n",
    "    \n",
    "    print(gdrive_url, file_path)\n",
    "    \n",
    "    !curl -L --progress-bar -o \"{file_path}\" \"{gdrive_url}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_file(file_path):\n",
    "    return subprocess.run(['shasum', file_path], stdout=subprocess.PIPE).stdout[:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8HlrLFjVcLS4",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'HG kouzine': (\n",
    "        '1dAeAt5Gu2cadwDhbc7OnenUgDLHlUvkx',\n",
    "        'hg_kouzine.pytorch_model.bin',\n",
    "    ),\n",
    "    'HG chipseq': (\n",
    "        '1VAsp8I904y_J0PUhAQqpSlCn1IqfG0FB',\n",
    "        'hg_chipseq.pytorch_model.bin',\n",
    "    ),\n",
    "    'MM curax': (\n",
    "        '1W6GEgHNoitlB-xXJbLJ_jDW4BF35W1Sd',\n",
    "        'mm_curax.pytorch_model.bin',\n",
    "    ),\n",
    "    'MM kouzine': (\n",
    "        '1dXpQFmheClKXIEoqcZ7kgCwx6hzVCv3H',\n",
    "        'mm_kouzine.pytorch_model.bin',\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './6-new-12w-0'\n",
    "model_data_path = './pytorch_models'\n",
    "target_model_data_path = os.path.join(data_path, 'pytorch_model.bin')\n",
    "output_path = 'output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_files = [\n",
    "    ('10sF8Ywktd96HqAL0CwvlZZUUGj05CGk5', os.path.join(data_path, 'config.json')),\n",
    "    ('16bT7HDv71aRwyh3gBUbKwign1mtyLD2d', os.path.join(data_path, 'special_tokens_map.json')),\n",
    "    ('1EE9goZ2JRSD8UTx501q71lGCk-CK3kqG', os.path.join(data_path, 'tokenizer_config.json')),\n",
    "    ('1gZZdtAoDnDiLQqjQfGyuwt268Pe5sXW0', os.path.join(data_path, 'vocab.txt')),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xYq4WVAtcLS4"
   },
   "outputs": [],
   "source": [
    "model_name_widget = widgets.Dropdown(\n",
    "    options=models.keys(),\n",
    "    value=next(iter(models.keys())),\n",
    "    description='model:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "model_confidence_threshold_widget = widgets.FloatText(\n",
    "    value=0.5,\n",
    "    description='model confidence threshold'\n",
    ")\n",
    "\n",
    "minimum_sequence_length_widget = widgets.IntText(\n",
    "    value=10,\n",
    "    description='minimum sequence length:',\n",
    ")\n",
    "\n",
    "check_sequence_variations_widget = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description='check reverse complement sequence variations'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasta_upload_widget = widgets.FileUpload(\n",
    "    multiple=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model_output = widgets.Output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@load_model_output.capture(clear_output=True)\n",
    "def load_model(btn):\n",
    "    global model_name, model_confidence_threshold, minimum_sequence_length, check_sequence_variations, model_file_path\n",
    "    \n",
    "    model_name = model_name_widget.value\n",
    "    model_confidence_threshold = model_confidence_threshold_widget.value\n",
    "    minimum_sequence_length = minimum_sequence_length_widget.value\n",
    "    check_sequence_variations = check_sequence_variations_widget.value\n",
    "    \n",
    "    model_gdrive_id, model_file_name = models[model_name]\n",
    "    \n",
    "    model_file_path = os.path.join(model_data_path, model_file_name)\n",
    "    \n",
    "    \n",
    "    print('downloading model data to input directory\\n')\n",
    "    \n",
    "    !mkdir $data_path\n",
    "    !mkdir $model_data_path\n",
    "    \n",
    "    gdown_wrapper(model_gdrive_id, model_file_path)\n",
    "    \n",
    "    for meta_file_gdrive_id, meta_file_file_path in meta_files:\n",
    "        gdown_wrapper(meta_file_gdrive_id, meta_file_file_path)\n",
    "    \n",
    "    \n",
    "    print('\\n\\ncopying model file to input directory\\n')\n",
    "    \n",
    "    hash1 = hash_file(model_file_path)\n",
    "    hash2 = hash_file(target_model_data_path)\n",
    "    print(hash1, hash2)\n",
    "    if hash1 != hash2:\n",
    "        !cp {model_file_path} {target_model_data_path}\n",
    "    \n",
    "    \n",
    "    print('\\n\\nloading model\\n')\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained(data_path)\n",
    "    model = BertForTokenClassification.from_pretrained(data_path)\n",
    "    is_cuda_available = torch.cuda.is_available()\n",
    "    print('cuda is', 'available' if is_cuda_available else 'not available')\n",
    "    if is_cuda_available:\n",
    "        model.cuda()\n",
    "    else:\n",
    "        model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model_button = widgets.Button(\n",
    "    description='Load model',\n",
    "    icon='truck-loading', # (FontAwesome names without the `fa-` prefix)\n",
    ")\n",
    "load_model_button.on_click(load_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir \"{output_path}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<button data-commandLinker-command=\"notebook:run-all-above\" class=\"lm-Widget jupyter-widgets jupyter-button\">Prepare environment</button>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select model and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(model_name_widget)\n",
    "display(model_confidence_threshold_widget)\n",
    "display(minimum_sequence_length_widget)\n",
    "display(check_sequence_variations_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(load_model_button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(load_model_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TAp3NQyaupxE"
   },
   "source": [
    "## Upload fasta files for prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "multiple files may be selected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(fasta_upload_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<button data-commandLinker-command=\"notebook:run-all-below\" class=\"lm-Widget jupyter-widgets jupyter-button\">Run prediction</button>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uploaded = {v['name']: v['content'] for v in fasta_upload_widget.value}\n",
    "for fn in uploaded.keys():\n",
    "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "      name=fn, length=len(uploaded[fn])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_1qjq3i-VCEz"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "out = []\n",
    "\n",
    "out.append('model_name: {}'.format(model_name))\n",
    "print('model_name: {}'.format(model_name))\n",
    "out.append('model_confidence: {}'.format(model_confidence_threshold))\n",
    "print('model_confidence: {}'.format(model_confidence_threshold))\n",
    "out.append('minimum_sequence_length: {}'.format(minimum_sequence_length))\n",
    "print('minimum_sequence_length: {}'.format(minimum_sequence_length))\n",
    "\n",
    "for key in uploaded.keys():\n",
    "    print(key)\n",
    "    out.append(key)\n",
    "    result_dict = {}\n",
    "    for seq_record in SeqIO.parse(StringIO(BytesIO(uploaded[key]).read().decode('UTF-8')), 'fasta'):\n",
    "        seqs = []\n",
    "        seq_uppered = str(seq_record.seq).upper()\n",
    "        seqs.append(('normal', seq_uppered))\n",
    "        if check_sequence_variations:\n",
    "            seq_uppered_complemented = complement_seq(seq_uppered)\n",
    "            #seqs.append(('complemented', seq_uppered_complemented))\n",
    "            #seq_uppered_reversed = reverse_seq(seq_uppered)\n",
    "            #seqs.append(('reversed', seq_uppered_reversed))\n",
    "            seq_uppered_complemented_reversed = reverse_seq(seq_uppered_complemented)\n",
    "            seqs.append(('reverse-complement', seq_uppered_complemented_reversed))\n",
    "        \n",
    "        print(seq_record.name)\n",
    "        out.append(seq_record.name)\n",
    "        \n",
    "        for seq_name, seq in seqs:\n",
    "            seq_key = '{}.{}.{}'.format(key, seq_record.name, seq_name)\n",
    "            \n",
    "            bed_out = []\n",
    "            \n",
    "            kmer_seq = seq2kmer(seq, 6)\n",
    "            seq_pieces = split_seq(kmer_seq)\n",
    "            print(seq_name)\n",
    "            out.append(seq_name)\n",
    "            with torch.no_grad():\n",
    "                preds = []\n",
    "                for seq_piece in tqdm(seq_pieces):\n",
    "                    input_ids = torch.LongTensor(tokenizer.encode(' '.join(seq_piece), add_special_tokens=False))\n",
    "                    input_ids_unsqueezed = None\n",
    "                    if is_cuda_available:\n",
    "                        input_ids_unsqueezed = input_ids.cuda().unsqueeze(0)\n",
    "                    else:\n",
    "                        input_ids_unsqueezed = input_ids.cpu().unsqueeze(0)\n",
    "                    outputs = torch.softmax(model(input_ids_unsqueezed)[-1],axis = -1)[0,:,1]\n",
    "                    preds.append(outputs.cpu().numpy())\n",
    "            result_dict[seq_key] = stitch_np_seq(preds)\n",
    "    \n",
    "    \n",
    "    \n",
    "            labeled, max_label = scipy.ndimage.label(result_dict[seq_key]>model_confidence_threshold)\n",
    "            print('  start     end')\n",
    "            out.append('  start     end')\n",
    "            for label in range(1, max_label+1):\n",
    "                candidate = np.where(labeled == label)[0]\n",
    "                candidate_length = candidate.shape[0]\n",
    "                if candidate_length>minimum_sequence_length:\n",
    "                    print('{:8}'.format(candidate[0]), '{:8}'.format(candidate[-1]))\n",
    "                    out.append('{:8}{:8}'.format(candidate[0], candidate[-1]))\n",
    "                    \n",
    "                    # start has to be subtracted by 1 for bed, see https://grch37.ensembl.org/info/website/upload/bed.html\n",
    "                    bed_out.append('0\\t{}\\t{}'.format(candidate[0] - 1, candidate[-1]))\n",
    "\n",
    "            with open(os.path.join(output_path, '{}.preds.pkl'.format(seq_key)),\"wb\") as fh:\n",
    "              pickle.dump(result_dict, fh)\n",
    "            print()\n",
    "            \n",
    "            bed_file_name = '{}.bed'.format(seq_key)\n",
    "            with open(os.path.join(output_path, bed_file_name),\"w\") as fh:\n",
    "                for item in bed_out:\n",
    "                    fh.write(\"%s\\n\" % item)\n",
    "\n",
    "with open(os.path.join(output_path, 'text_predictions.txt'),\"w\") as fh:\n",
    "    for item in out:\n",
    "        fh.write(\"%s\\n\" % item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "ZDNA prediction",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
