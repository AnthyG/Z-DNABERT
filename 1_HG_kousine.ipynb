{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dumerenkov/anaconda3/lib/python3.7/site-packages/pandas/compat/_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/home/dumerenkov/anaconda3/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "import random\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from tqdm import trange\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import os\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from joblib import Parallel, delayed, dump, load\n",
    "from matplotlib import pyplot as plt\n",
    "from sparse_vector.sparse_vector import SparseVector\n",
    "from scipy.signal import convolve2d, convolve\n",
    "import time\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ZDNA_kouzine = {}\n",
    "#ZDNA = load(f'../data/hg19_zdna/sparse/ZDNA_2016.pkl')\n",
    "#for chrom in ZDNA:\n",
    "#    ZDNA_kouzine[chrom] = np.zeros(ZDNA[chrom].shape, dtype = bool)\n",
    "#    \n",
    "#    \n",
    "#with open(\"Kouzine_hg19.bed\")as f:\n",
    "#    for idx, line in enumerate(f):\n",
    "#        if idx>0:\n",
    "#            chrom, start, end = line.strip().split()\n",
    "#            if chrom in ZDNA_kouzine:\n",
    "#                ZDNA_kouzine[chrom][int(start):int(end)] = 1\n",
    "#\n",
    "#dump(ZDNA_kouzine, 'ZDNA_hg19_kouzine.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASSEMBLY_d = {}\n",
    "chroms_d = {}\n",
    "all_features_d = {}\n",
    "groups_d = {}\n",
    "feature_names_d = {}\n",
    "ZDNA_d = {}\n",
    "black_list_d = {}\n",
    "DNA_d = {}\n",
    "DNA_features_d = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MM9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASSEMBLY = \"curax_14h_UNI_mm9\"\n",
    "chroms = [f'chr{i}' for i in list(range(1, 20)) + ['X', 'Y']]\n",
    "all_features = sorted([i[:-4] for i in os.listdir('../data/mm9_features/sparse/') if i.endswith('.pkl')])\n",
    "groups = ['DNase-seq', 'Histone', 'RNA polymerase', 'TFs and others']\n",
    "feature_names = [i for i in all_features if (i.split('_')[0] in groups)]\n",
    "ZDNA = load(f'../data/mm9_zdna/sparse/{ASSEMBLY}.pkl')\n",
    "black_list = load(f'../data/mm9_zdna/sparse/blacklist_mm9.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [01:04<00:00,  3.06s/it]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 873/873 [03:22<00:00,  4.30it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 873/873 [00:00<00:00, 143272.97it/s]\n"
     ]
    }
   ],
   "source": [
    "DNA = {chrom:load(f'../data/mm9_dna/sparse/{chrom}.pkl') for chrom in tqdm(chroms)}\n",
    "\n",
    "DNA_features = {feture: load(f'../data/mm9_features/sparse/{feture}.pkl')\n",
    "                for feture in tqdm(feature_names)}\n",
    "\n",
    "for feature in tqdm(DNA_features):\n",
    "    if set(DNA_features[feature].keys()) != set(chroms):\n",
    "        for chrom in chroms:\n",
    "            if chrom not in DNA_features[feature]:\n",
    "                DNA_features[feature][chrom] = SparseVector(len(DNA[chrom]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'mm9'\n",
    "ASSEMBLY_d[mode] = ASSEMBLY\n",
    "chroms_d[mode] = chroms\n",
    "all_features_d[mode] = all_features\n",
    "groups_d[mode] = groups\n",
    "feature_names_d[mode] = feature_names\n",
    "ZDNA_d[mode] = ZDNA\n",
    "black_list_d[mode] = black_list\n",
    "DNA_d[mode] = DNA\n",
    "DNA_features_d[mode] = DNA_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASSEMBLY = \"ZDNA_2016\"\n",
    "chroms = [f'chr{i}' for i in list(range(1, 23)) + ['X', 'Y']]\n",
    "all_features = sorted([i[:-4] for i in os.listdir('../data/hg19_features/sparse/') if i.endswith('.pkl')])\n",
    "groups = ['DNase-seq', 'Histone', 'RNA polymerase', 'TFs and others']\n",
    "feature_names = [i for i in all_features if (i.split('_')[0] in groups)]\n",
    "ZDNA = load('ZDNA_hg19_kouzine.pkl')\n",
    "black_list = load(f'../data/hg19_zdna/sparse/blacklist_hg19.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [05:17<00:00, 13.24s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1054/1054 [01:55<00:00,  9.15it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 1054/1054 [00:00<00:00, 213864.66it/s]\n"
     ]
    }
   ],
   "source": [
    "def chrom_reader(chrom):\n",
    "    files = sorted([i for i in os.listdir(f'../data/hg19_dna/') if f\"{chrom}_\" in i])\n",
    "    return ''.join([load(f\"../data/hg19_dna/{file}\") for file in files])\n",
    "\n",
    "\n",
    "DNA = {chrom:chrom_reader(chrom) for chrom in tqdm(chroms)}\n",
    "\n",
    "DNA_features = {feture: load(f'../data/hg19_features/sparse/{feture}.pkl')\n",
    "                for feture in tqdm(feature_names)}\n",
    "\n",
    "for feature in tqdm(DNA_features):\n",
    "    if set(DNA_features[feature].keys()) != set(chroms):\n",
    "        for chrom in chroms:\n",
    "            if chrom not in DNA_features[feature]:\n",
    "                DNA_features[feature][chrom] = SparseVector(len(DNA[chrom]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'hg19'\n",
    "ASSEMBLY_d[mode] = ASSEMBLY\n",
    "chroms_d[mode] = chroms\n",
    "all_features_d[mode] = all_features\n",
    "groups_d[mode] = groups\n",
    "feature_names_d[mode] = feature_names\n",
    "ZDNA_d[mode] = ZDNA\n",
    "black_list_d[mode] = black_list\n",
    "DNA_d[mode] = DNA\n",
    "DNA_features_d[mode] = DNA_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersect = {i.upper() for i in DNA_features_d['mm9']} & {i.upper() for i in DNA_features_d['hg19']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_bed(source, name):\n",
    "    buf = []\n",
    "    for chrm in source[name]:\n",
    "        beds = source[name][chrm].indices[source[name][chrm].data != 0]\n",
    "        data = source[name][chrm].data[source[name][chrm].data != 0].astype(float).astype(int)\n",
    "        ends = np.append(source[name][chrm].indices[1:], \n",
    "              [source[name][chrm].shape])[source[name][chrm].data != 0]\n",
    "\n",
    "        buf.extend([[chrm, beds[i], ends[i], data[i]] for i in range(len(beds))])\n",
    "    buf = np.array(buf)\n",
    "    df = pd.DataFrame([buf[:, 0], buf[:, 1], buf[:, 2], '', buf[:, 3], \"\", buf[:, 1], buf[:, 2], \"\"]).T\n",
    "    df[3] = ''\n",
    "    df[5] = \"+\"\n",
    "    df[8] = ''\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 470/470 [31:31<00:00,  4.02s/it]\n"
     ]
    }
   ],
   "source": [
    "gen = 'hg19'\n",
    "\n",
    "for name in tqdm([i for i in DNA_features_d[gen] if i.upper() in intersect and 'TFs and others' in i]):\n",
    "    df = to_bed(DNA_features_d[gen], name)\n",
    "    df.to_csv(f'tmp/{gen}_{name}.bed', sep = '\\t', index=False, header=None)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(DNA_features_d, name, gen):\n",
    "    df = to_bed(DNA_features_d, name)\n",
    "    df.to_csv(f'tmp/{gen}_{name}.bed', sep = '\\t', index=False, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|█████████████████████████████████████████████▊                                                                  | 192/470 [09:18<17:38,  3.81s/it]/home/dumerenkov/anaconda3/lib/python3.7/site-packages/pandas/compat/_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/home/dumerenkov/anaconda3/lib/python3.7/site-packages/pandas/compat/_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n",
      " 51%|█████████████████████████████████████████████████████████▏                                                      | 240/470 [13:53<17:14,  4.50s/it]/home/dumerenkov/anaconda3/lib/python3.7/site-packages/pandas/compat/_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/home/dumerenkov/anaconda3/lib/python3.7/site-packages/pandas/compat/_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n",
      " 61%|████████████████████████████████████████████████████████████████████▋                                           | 288/470 [18:25<14:51,  4.90s/it]/home/dumerenkov/anaconda3/lib/python3.7/site-packages/pandas/compat/_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 470/470 [31:58<00:00,  4.08s/it]\n",
      "/home/dumerenkov/anaconda3/lib/python3.7/site-packages/pandas/compat/_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/home/dumerenkov/anaconda3/lib/python3.7/site-packages/pandas/compat/_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/home/dumerenkov/anaconda3/lib/python3.7/site-packages/pandas/compat/_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/home/dumerenkov/anaconda3/lib/python3.7/site-packages/pandas/compat/_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/home/dumerenkov/anaconda3/lib/python3.7/site-packages/pandas/compat/_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "gen = 'hg19'\n",
    "Parallel(n_jobs = -1)(delayed(func)(DNA_features_d[gen], name, gen)\n",
    "                     for name in tqdm([i for i in DNA_features_d[gen] \n",
    "                                       if i.upper() in intersect and 'TFs and others' in i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'hg19'\n",
    "ASSEMBLY = ASSEMBLY_d[mode]\n",
    "chroms = chroms_d[mode]\n",
    "all_features = all_features_d[mode]\n",
    "groups = groups_d[mode]\n",
    "feature_names = feature_names_d[mode]\n",
    "ZDNA = ZDNA_d[mode]\n",
    "black_list = black_list_d[mode]\n",
    "DNA = DNA_d[mode]\n",
    "DNA_features = DNA_features_d[mode]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 486817/486817 [00:18<00:00, 25950.72it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 474998/474998 [00:18<00:00, 25731.71it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 386762/386762 [00:14<00:00, 25931.55it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 373348/373348 [00:14<00:00, 25992.98it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 353350/353350 [00:13<00:00, 25420.42it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 334209/334209 [00:12<00:00, 26183.16it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 310817/310817 [00:11<00:00, 26498.00it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 285867/285867 [00:10<00:00, 26040.83it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 275807/275807 [00:10<00:00, 25760.91it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 264716/264716 [00:09<00:00, 27970.24it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 263684/263684 [00:10<00:00, 25191.71it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 261429/261429 [00:09<00:00, 26964.03it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 224941/224941 [00:08<00:00, 26923.48it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 209667/209667 [00:07<00:00, 27640.89it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 200256/200256 [00:08<00:00, 24554.06it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 176474/176474 [00:06<00:00, 26846.57it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 158584/158584 [00:05<00:00, 26847.01it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 152494/152494 [00:05<00:00, 26423.42it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 115486/115486 [00:04<00:00, 27867.03it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 123096/123096 [00:04<00:00, 27595.75it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 94003/94003 [00:03<00:00, 26453.17it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 100204/100204 [00:03<00:00, 26836.46it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 303262/303262 [00:11<00:00, 26897.42it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 115963/115963 [00:04<00:00, 27635.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29745\n",
      "5443119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "width = 512\n",
    "\n",
    "np.random.seed(10)\n",
    "\n",
    "ints_in = []\n",
    "ints_out = []\n",
    "\n",
    "\n",
    "for chrm in chroms:\n",
    "    for st in trange(0, ZDNA[chrm].shape[0] - width, width):\n",
    "        interval = [st, min(st + width, ZDNA[chrm].shape[0])]\n",
    "        N_count = sum([bp == \"N\" for bp in DNA[chrm][interval[0]:interval[1]]])\n",
    "        bl_count = black_list[chrm][interval[0]:interval[1]].sum()\n",
    "        if N_count > width / 2 or bl_count > 0:\n",
    "            continue\n",
    "        else:\n",
    "            if ZDNA[chrm][interval[0]: interval[1]].any():\n",
    "                ints_in.append([chrm, int(interval[0]), int(interval[1]), 1])\n",
    "            else:\n",
    "                ints_out.append([chrm, int(interval[0]), int(interval[1]), 0])\n",
    "\n",
    "\n",
    "                \n",
    "                \n",
    "print(len(ints_in))\n",
    "print(len(ints_out))\n",
    "\n",
    "ints_in_full = ints_in\n",
    "ints_out_full = ints_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29745\n",
      "59490\n"
     ]
    }
   ],
   "source": [
    "ints_in = ints_in_full\n",
    "ints_out = [ints_out_full[i] for i in np.random.choice(range(len(ints_out_full)), \n",
    "                                                    size=len(ints_in) * 2, replace=False)]\n",
    "# ints_out = ints_out_full\n",
    "\n",
    "print(len(ints_in))\n",
    "print(len(ints_out))\n",
    "#484 for len 1000 \n",
    "#9680 for len 1000\n",
    "\n",
    "#629 for len 512\n",
    "#12580 for len 5121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "equalized = ints_in + ints_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divisions = list(StratifiedKFold(5, shuffle=True, \n",
    "                                 random_state=42).split(equalized, [f\"{elem[3]}_{elem[0]}\"\n",
    "                                         for i, elem \n",
    "                                         in enumerate(equalized)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump([equalized, divisions], 'hg_divisions_kouzine.pkl', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2kmer(seq, k):\n",
    "    \"\"\"\n",
    "    Convert original sequence to kmers\n",
    "    \n",
    "    Arguments:\n",
    "    seq -- str, original sequence.\n",
    "    k -- int, kmer of length k specified.\n",
    "    \n",
    "    Returns:\n",
    "    kmers -- str, kmers separated by space\n",
    "    \"\"\"\n",
    "    kmer = [seq[x:x+k] for x in range(len(seq)+1-k)]\n",
    "    kmers = \" \".join(kmer)\n",
    "    return kmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, chroms, features, \n",
    "                 dna_source, features_source, \n",
    "                 labels_source, intervals, tokenizer):\n",
    "\n",
    "        self.chroms = chroms\n",
    "        self.features = features\n",
    "        self.dna_source = dna_source\n",
    "        self.features_source = features_source\n",
    "        self.labels_source = labels_source\n",
    "        self.intervals = intervals\n",
    "        self.le = LabelBinarizer().fit(np.array([[\"A\"], [\"C\"], [\"T\"], [\"G\"]]))\n",
    "        self.configs = {\n",
    "                        'ZHUNT_AS': {\n",
    "                                'CG': 0, 'GC': 1, 'CA': 0, 'AC': 1, \n",
    "                                'TG': 0, 'GT': 1, 'TA': 1, 'AT': 1, \n",
    "                                'CC': 0, 'GG': 0, 'CT': 1, 'TC': 1, \n",
    "                                'GA': 1, 'AG': 1, 'AA': 1, 'TT': 1},\n",
    "                       }\n",
    "        seqs = ([\"A\", \"C\", \"T\", \"G\"] + \n",
    "                ['AC', 'AT', 'AG', 'CT', 'CG', 'GT'] +\n",
    "                ['AAC', 'ACC', 'AAT', 'ATT', 'AAG', 'AGG', \n",
    "                 'CCA', 'CAA', 'CCT', 'CTT', 'CCG', 'CGG', \n",
    "                 'TTA', 'TAA', 'TTC', 'TCC', 'TTG', 'TGG', \n",
    "                 'GGA', 'GAA', 'GGC', 'GCC', 'GGT', 'GTT'] +\n",
    "                ['AAAC', 'AAAT', 'AAAG', 'CCCA', 'CCCT', 'CCCG',\n",
    "                 'TTTA', 'TTTC', 'TTTG', 'GGGA', 'GGGC', 'GGGT'])\n",
    "        self.tars = np.array([self.le.transform(list(i * 11)[:11]) for i in seqs])[:, ::-1, ::-1]\n",
    "        # purine-pyrimidine\n",
    "        self.tars = np.concatenate((self.tars, np.array([self.tars[4] + self.tars[9]])))\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.intervals)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        interval = self.intervals[index]\n",
    "        chrom = interval[0]\n",
    "        begin = int(interval[1])\n",
    "        end = int(interval[2])\n",
    "        ll = list(self.dna_source[chrom][begin:end].upper())\n",
    "        y = self.labels_source[interval[0]][interval[1]: interval[2]]        \n",
    "        \n",
    "        \n",
    "#         DNA PART\n",
    "        \n",
    "        dna_OHE = self.le.transform(ll)[None]\n",
    "        \n",
    "        res = pd.DataFrame(convolve(dna_OHE, self.tars)[:, 5:-5, 3].T / 11)\n",
    "        res = (res.rolling(5, min_periods=1).max().values == 1).astype(int)\n",
    "        \n",
    "        \n",
    "#         ZHUNT PART\n",
    "        zhunts = []\n",
    "        for key in self.configs:\n",
    "            vec = np.array(ll)\n",
    "            vec = np.vectorize(lambda x:self.configs[key].get(x, 0))(\n",
    "                                    np.char.add(vec[1:], vec[:-1]))\n",
    "            zhunts.append(np.concatenate([vec, [0]]))\n",
    "        \n",
    "        \n",
    "        # FEATURES PART\n",
    "        feature_matr = []\n",
    "        for feature in self.features:\n",
    "            source = self.features_source[feature]\n",
    "            feature_matr.append(source[chrom][begin:end])\n",
    "        \n",
    "        # UNION\n",
    "        if len(feature_matr) > 0:\n",
    "            X = np.hstack((\n",
    "                           res,\n",
    "                           np.array(zhunts).T, \n",
    "                           np.array(feature_matr).T/1000)).astype(np.float32)\n",
    "#             X = (np.array(feature_matr).T/1000).astype(np.float32)\n",
    "        else:\n",
    "            X = dna_OHE.astype(np.float32)\n",
    "        \n",
    "        #K-mer part\n",
    "        \n",
    "        k_mers = seq2kmer(self.dna_source[chrom][begin:end+5].upper(),6)\n",
    "        encoded_k_mers = self.tokenizer.encode_plus(k_mers, add_special_tokens=False, max_length=512)[\"input_ids\"]\n",
    "\n",
    "        return torch.Tensor(X), torch.Tensor(y).long(), ll, torch.LongTensor(encoded_k_mers), (chrom, begin, end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertConfig, PreTrainedTokenizer, BasicTokenizer, BertForTokenClassification\n",
    "import collections\n",
    "from torch.utils.data import DataLoader\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_FILES_NAMES = {\"vocab_file\": \"vocab.txt\"}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "PRETRAINED_VOCAB_FILES_MAP = {\"vocab_file\": {\"dna3\": \"https://raw.githubusercontent.com/jerryji1993/DNABERT/master/src/transformers/dnabert-config/bert-config-3/vocab.txt\",\n",
    "                                             \"dna4\": \"https://raw.githubusercontent.com/jerryji1993/DNABERT/master/src/transformers/dnabert-config/bert-config-4/vocab.txt\",\n",
    "                                             \"dna5\": \"https://raw.githubusercontent.com/jerryji1993/DNABERT/master/src/transformers/dnabert-config/bert-config-5/vocab.txt\",\n",
    "                                             \"dna6\": \"https://raw.githubusercontent.com/jerryji1993/DNABERT/master/src/transformers/dnabert-config/bert-config-6/vocab.txt\"}}\n",
    "\n",
    "\n",
    "\n",
    "PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n",
    "                                          \"dna3\": 512,\n",
    "                                          \"dna4\": 512,\n",
    "                                          \"dna5\": 512,\n",
    "                                          \"dna6\": 512}\n",
    "\n",
    "PRETRAINED_INIT_CONFIGURATION = {\n",
    "    \"dna3\": {\"do_lower_case\": False},\n",
    "    \"dna4\": {\"do_lower_case\": False},\n",
    "    \"dna5\": {\"do_lower_case\": False},\n",
    "    \"dna6\": {\"do_lower_case\": False}}\n",
    "\n",
    "VOCAB_KMER = {\n",
    "    \"69\": \"3\",\n",
    "    \"261\": \"4\",\n",
    "    \"1029\": \"5\",\n",
    "    \"4101\": \"6\",}\n",
    "\n",
    "\n",
    "def load_vocab(vocab_file):\n",
    "    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
    "    vocab = collections.OrderedDict()\n",
    "    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n",
    "        tokens = reader.readlines()\n",
    "    for index, token in enumerate(tokens):\n",
    "        token = token.rstrip(\"\\n\")\n",
    "        vocab[token] = index\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def whitespace_tokenize(text):\n",
    "    \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "class DNATokenizer(PreTrainedTokenizer):\n",
    "    r\"\"\"\n",
    "    Constructs a BertTokenizer.\n",
    "    :class:`~transformers.BertTokenizer` runs end-to-end tokenization: punctuation splitting + wordpiece\n",
    "    Args:\n",
    "        vocab_file: Path to a one-wordpiece-per-line vocabulary file\n",
    "        do_lower_case: Whether to lower case the input. Only has an effect when do_basic_tokenize=True\n",
    "        do_basic_tokenize: Whether to do basic tokenization before wordpiece.\n",
    "        max_len: An artificial maximum length to truncate tokenized sequences to; Effective maximum length is always the\n",
    "            minimum of this value (if specified) and the underlying BERT model's sequence length.\n",
    "        never_split: List of tokens which will never be split during tokenization. Only has an effect when\n",
    "            do_basic_tokenize=True\n",
    "    \"\"\"\n",
    "\n",
    "    vocab_files_names = VOCAB_FILES_NAMES\n",
    "    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n",
    "    pretrained_init_configuration = PRETRAINED_INIT_CONFIGURATION\n",
    "    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n",
    "\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_file,\n",
    "        do_lower_case=False,\n",
    "        never_split=None,\n",
    "        unk_token=\"[UNK]\",\n",
    "        sep_token=\"[SEP]\",\n",
    "        pad_token=\"[PAD]\",\n",
    "        cls_token=\"[CLS]\",\n",
    "        mask_token=\"[MASK]\",\n",
    "        tokenize_chinese_chars=True,\n",
    "        max_len = 512,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"Constructs a BertTokenizer.\n",
    "        Args:\n",
    "            **vocab_file**: Path to a one-wordpiece-per-line vocabulary file\n",
    "            **do_lower_case**: (`optional`) boolean (default True)\n",
    "                Whether to lower case the input\n",
    "                Only has an effect when do_basic_tokenize=True\n",
    "            **do_basic_tokenize**: (`optional`) boolean (default True)\n",
    "                Whether to do basic tokenization before wordpiece.\n",
    "            **never_split**: (`optional`) list of string\n",
    "                List of tokens which will never be split during tokenization.\n",
    "                Only has an effect when do_basic_tokenize=True\n",
    "            **tokenize_chinese_chars**: (`optional`) boolean (default True)\n",
    "                Whether to tokenize Chinese characters.\n",
    "                This should likely be deactivated for Japanese:\n",
    "                see: https://github.com/huggingface/pytorch-pretrained-BERT/issues/328\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            unk_token=unk_token,\n",
    "            sep_token=sep_token,\n",
    "            pad_token=pad_token,\n",
    "            cls_token=cls_token,\n",
    "            mask_token=mask_token,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.vocab = load_vocab(vocab_file)\n",
    "        self.max_len = max_len\n",
    "        #self.max_len_single_sentence = self.max_len - 2  # take into account special tokens\n",
    "        #self.max_len_sentences_pair = self.max_len - 3  # take into account special tokens\n",
    "\n",
    "        if not os.path.isfile(vocab_file):\n",
    "            raise ValueError(\n",
    "                \"Can't find a vocabulary file at path '{}'. To load the vocabulary from a Google pretrained \"\n",
    "                \"model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\".format(vocab_file)\n",
    "            )\n",
    "        \n",
    "        self.kmer = VOCAB_KMER[str(len(self.vocab))]\n",
    "        self.ids_to_tokens = collections.OrderedDict([(ids, tok) for tok, ids in self.vocab.items()])\n",
    "        self.basic_tokenizer = BasicTokenizer(\n",
    "                do_lower_case=do_lower_case, never_split=never_split, tokenize_chinese_chars=tokenize_chinese_chars\n",
    "            )\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.vocab)\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        split_tokens = []\n",
    "        for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
    "                split_tokens.append(token)\n",
    "        # print(split_tokens)\n",
    "        return split_tokens\n",
    "\n",
    "    def _convert_token_to_id(self, token):\n",
    "        \"\"\" Converts a token (str) in an id using the vocab. \"\"\"\n",
    "        return self.vocab.get(token, self.vocab.get(self.unk_token))\n",
    "\n",
    "    def _convert_id_to_token(self, index):\n",
    "        \"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\n",
    "        return self.ids_to_tokens.get(index, self.unk_token)\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        \"\"\" Converts a sequence of tokens (string) in a single string. \"\"\"\n",
    "        out_string = \" \".join(tokens).replace(\" ##\", \"\").strip()\n",
    "        return out_string\n",
    "\n",
    "    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
    "        \"\"\"\n",
    "        Build model inputs from a sequence or a pair of sequence for sequence classification tasks\n",
    "        by concatenating and adding special tokens.\n",
    "        A BERT sequence has the following format:\n",
    "            single sequence: [CLS] X [SEP]\n",
    "            pair of sequences: [CLS] A [SEP] B [SEP]\n",
    "        \"\"\"\n",
    "        cls = [self.cls_token_id]\n",
    "        sep = [self.sep_token_id]\n",
    "\n",
    "        if token_ids_1 is None:\n",
    "            if len(token_ids_0) < 510:\n",
    "                return cls + token_ids_0 + sep\n",
    "            else:\n",
    "                output = []\n",
    "                num_pieces = int(len(token_ids_0)//510) + 1\n",
    "                for i in range(num_pieces):\n",
    "                    output.extend(cls + token_ids_0[510*i:min(len(token_ids_0), 510*(i+1))] + sep)\n",
    "                return output\n",
    "\n",
    "        return cls + token_ids_0 + sep + token_ids_1 + sep\n",
    "\n",
    "    def get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_special_tokens=False):\n",
    "        \"\"\"\n",
    "        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n",
    "        special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\n",
    "        Args:\n",
    "            token_ids_0: list of ids (must not contain special tokens)\n",
    "            token_ids_1: Optional list of ids (must not contain special tokens), necessary when fetching sequence ids\n",
    "                for sequence pairs\n",
    "            already_has_special_tokens: (default False) Set to True if the token list is already formated with\n",
    "                special tokens for the model\n",
    "        Returns:\n",
    "            A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n",
    "        \"\"\"\n",
    "\n",
    "        if already_has_special_tokens:\n",
    "            if token_ids_1 is not None:\n",
    "                raise ValueError(\n",
    "                    \"You should not supply a second sequence if the provided sequence of \"\n",
    "                    \"ids is already formated with special tokens for the model.\"\n",
    "                )\n",
    "            return list(map(lambda x: 1 if x in [self.sep_token_id, self.cls_token_id] else 0, token_ids_0))\n",
    "\n",
    "        if token_ids_1 is not None:\n",
    "            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n",
    "        \n",
    "        if len(token_ids_0) < 510:\n",
    "            return [1] + ([0] * len(token_ids_0)) + [1]\n",
    "        else:\n",
    "            output = []\n",
    "            num_pieces = int(len(token_ids_0)//510) + 1\n",
    "            for i in range(num_pieces):\n",
    "                output.extend([1] + ([0] * (min(len(token_ids_0), 510*(i+1))-510*i)) + [1])\n",
    "            return output\n",
    "            return [1] + ([0] * len(token_ids_0)) + [1]\n",
    "\n",
    "    def create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1=None):\n",
    "        \"\"\"\n",
    "        Creates a mask from the two sequences passed to be used in a sequence-pair classification task.\n",
    "        A BERT sequence pair mask has the following format:\n",
    "        0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
    "        | first sequence    | second sequence\n",
    "        if token_ids_1 is None, only returns the first portion of the mask (0's).\n",
    "        \"\"\"\n",
    "        sep = [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "        if token_ids_1 is None:\n",
    "            if len(token_ids_0) < 510:\n",
    "                return len(cls + token_ids_0 + sep) * [0]\n",
    "            else:\n",
    "                num_pieces = int(len(token_ids_0)//510) + 1\n",
    "                return (len(cls + token_ids_0 + sep) + 2*(num_pieces-1)) * [0]\n",
    "        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n",
    "\n",
    "    def save_vocabulary(self, vocab_path):\n",
    "        \"\"\"Save the tokenizer vocabulary to a directory or file.\"\"\"\n",
    "        index = 0\n",
    "        if os.path.isdir(vocab_path):\n",
    "            vocab_file = os.path.join(vocab_path, VOCAB_FILES_NAMES[\"vocab_file\"])\n",
    "        else:\n",
    "            vocab_file = vocab_path\n",
    "        with open(vocab_file, \"w\", encoding=\"utf-8\") as writer:\n",
    "            for token, token_index in sorted(self.vocab.items(), key=lambda kv: kv[1]):\n",
    "                if index != token_index:\n",
    "                    logger.warning(\n",
    "                        \"Saving vocabulary to {}: vocabulary indices are not consecutive.\"\n",
    "                        \" Please check that the vocabulary is not corrupted!\".format(vocab_file)\n",
    "                    )\n",
    "                    index = token_index\n",
    "                writer.write(token + \"\\n\")\n",
    "                index += 1\n",
    "        return (vocab_file,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'DNATokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'DNATokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'DNATokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'DNATokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'DNATokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'DNATokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'DNATokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'DNATokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'DNATokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'DNATokenizer'.\n"
     ]
    }
   ],
   "source": [
    "for MODEL_NUMBER in range(5):\n",
    "\n",
    "    train_inds, test_inds = divisions[MODEL_NUMBER]\n",
    "    train_intervals, test_intervals = [equalized[i] for i in train_inds], [equalized[i] for i in test_inds]\n",
    "\n",
    "    random.shuffle(train_intervals)\n",
    "    random.shuffle(test_intervals)\n",
    "    \n",
    "    train_dataset = Dataset(chroms, \n",
    "                        [i for i in feature_names if i.upper() in intersect], \n",
    "                       DNA, DNA_features, ZDNA, train_intervals, \n",
    "                        DNATokenizer.from_pretrained('6-new-12w-0/', add_special_tokens=False))\n",
    "\n",
    "    test_dataset = Dataset(chroms, \n",
    "                       [i for i in feature_names if i.upper() in intersect], \n",
    "                       DNA, DNA_features, ZDNA, test_intervals,\n",
    "                          DNATokenizer.from_pretrained('6-new-12w-0/', add_special_tokens=False))\n",
    "    \n",
    "    dump((train_dataset, test_dataset), f'ds_w_seq_hg_fold{MODEL_NUMBER}_kouzine.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    tr_loss, tr_accuracy = 0, 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    tr_preds, tr_labels = [], []\n",
    "    # put model in training mode\n",
    "    model.train()\n",
    "    \n",
    "    for idx, batch in enumerate(training_loader):\n",
    "        features, labels, sequences, input_ids, intervals = batch            \n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device)\n",
    "        #print(model.device, input_ids.device, labels.device)\n",
    "        outputs = model(input_ids = input_ids, labels = labels)        \n",
    "        #print(outputs)\n",
    "        loss, tr_logits = outputs['loss'], outputs['logits']\n",
    "        #print(model(input_ids=ids, attention_mask=mask, labels=labels))\n",
    "        tr_loss += loss.item()\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples += labels.size(0)\n",
    "        \n",
    "        if idx % 1000==0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            print(f\"Training loss per 1000 training steps: {loss_step}\")\n",
    "           \n",
    "        # compute training accuracy\n",
    "        flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n",
    "        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "        \n",
    "        # only compute accuracy at active labels\n",
    "        active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
    "        active_labels = torch.where(active_accuracy, labels.view(-1), torch.tensor(-100).type_as(labels))\n",
    "        \n",
    "        labels = torch.masked_select(flattened_targets, active_accuracy)\n",
    "        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "        \n",
    "        tr_labels.extend(labels)\n",
    "        tr_preds.extend(predictions)\n",
    "\n",
    "        tmp_tr_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "        tr_accuracy += tmp_tr_accuracy\n",
    "    \n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            parameters=model.parameters(), max_norm=0.1\n",
    "        )\n",
    "        \n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    epoch_loss = tr_loss / nb_tr_steps\n",
    "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
    "    print(f\"Training loss epoch: {epoch_loss}\")\n",
    "    print(f\"Training accuracy epoch: {tr_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, testing_loader):\n",
    "    # put model in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_examples, nb_eval_steps = 0, 0\n",
    "    eval_preds, eval_labels, eval_scores = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(testing_loader):\n",
    "            \n",
    "            features, labels, sequences, input_ids, intervals = batch            \n",
    "            input_ids = input_ids.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(input_ids = input_ids, labels = labels)\n",
    "            loss, eval_logits = outputs['loss'], outputs['logits']            \n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            nb_eval_steps += 1\n",
    "            nb_eval_examples += labels.size(0)\n",
    "        \n",
    "            if idx % 100==0:\n",
    "                loss_step = eval_loss/nb_eval_steps\n",
    "                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n",
    "              \n",
    "            # compute evaluation accuracy\n",
    "            flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n",
    "            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
    "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
    "            flattened_scores = active_logits[:,1] - active_logits[:,0]\n",
    "            \n",
    "            # only compute accuracy at active labels\n",
    "            active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n",
    "        \n",
    "            labels = torch.masked_select(flattened_targets, active_accuracy)\n",
    "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
    "            \n",
    "            eval_labels.extend(labels)\n",
    "            eval_preds.extend(predictions)\n",
    "            eval_scores.extend(flattened_scores)\n",
    "            \n",
    "            tmp_eval_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "    labels = [id.item() for id in eval_labels]\n",
    "    predictions = [id.item() for id in eval_preds]\n",
    "    scores = [id.item() for id in eval_scores]\n",
    "    \n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    eval_accuracy = eval_accuracy / nb_eval_steps\n",
    "    print(f\"Validation Loss: {eval_loss}\")\n",
    "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
    "\n",
    "    return labels, predictions, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'DNATokenizer'.\n",
      "Some weights of the model checkpoint at 6-new-12w-0/ were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at 6-new-12w-0/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 0 epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss per 1000 training steps: 0.8366020321846008\n",
      "Training loss per 1000 training steps: 0.2869319598917123\n",
      "Training loss per 1000 training steps: 0.16218111004230326\n",
      "Training loss epoch: 0.11599912576279564\n",
      "Training accuracy epoch: 0.9430947183560932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss per 100 evaluation steps: 0.0043183728121221066\n",
      "Validation loss per 100 evaluation steps: 0.016173384203068394\n",
      "Validation loss per 100 evaluation steps: 0.01759079828469166\n",
      "Validation loss per 100 evaluation steps: 0.017340146148062167\n",
      "Validation loss per 100 evaluation steps: 0.01731762515965668\n",
      "Validation loss per 100 evaluation steps: 0.017553821093046817\n",
      "Validation loss per 100 evaluation steps: 0.017605893777899548\n",
      "Validation loss per 100 evaluation steps: 0.01764459572421897\n",
      "Validation loss per 100 evaluation steps: 0.017854657736616705\n",
      "Validation loss per 100 evaluation steps: 0.017718566188067195\n",
      "Validation loss per 100 evaluation steps: 0.01770958976592485\n",
      "Validation loss per 100 evaluation steps: 0.017857526429651757\n",
      "Validation Loss: 0.017854086029543123\n",
      "Validation Accuracy: 0.9923857939408122\n",
      "Fold 0 validation ROC-AUC:  0.9972264205410716\n",
      "Training fold 0 epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss per 1000 training steps: 0.017959607765078545\n",
      "Training loss per 1000 training steps: 0.01853669011207357\n",
      "Training loss per 1000 training steps: 0.01763798286328497\n",
      "Training loss epoch: 0.017128826313526878\n",
      "Training accuracy epoch: 0.9928434108018247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss per 100 evaluation steps: 0.002429909771308303\n",
      "Validation loss per 100 evaluation steps: 0.013649954576976597\n",
      "Validation loss per 100 evaluation steps: 0.01479731196252657\n",
      "Validation loss per 100 evaluation steps: 0.01474373962689946\n",
      "Validation loss per 100 evaluation steps: 0.014653535040114029\n",
      "Validation loss per 100 evaluation steps: 0.014723175471814018\n",
      "Validation loss per 100 evaluation steps: 0.014791936121614474\n",
      "Validation loss per 100 evaluation steps: 0.014776997997061968\n",
      "Validation loss per 100 evaluation steps: 0.014960657546932876\n",
      "Validation loss per 100 evaluation steps: 0.014855135895422578\n",
      "Validation loss per 100 evaluation steps: 0.014863846619753623\n",
      "Validation loss per 100 evaluation steps: 0.01501106990700207\n",
      "Validation Loss: 0.015014397627203909\n",
      "Validation Accuracy: 0.9937358959723421\n",
      "Fold 0 validation ROC-AUC:  0.9980448894387864\n",
      "Training fold 0 epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss per 1000 training steps: 0.01409136038273573\n",
      "Training loss per 1000 training steps: 0.015568088632126754\n",
      "Training loss per 1000 training steps: 0.015153457993524673\n",
      "Training loss epoch: 0.01504888799933701\n",
      "Training accuracy epoch: 0.9937319185486692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss per 100 evaluation steps: 0.0024784430861473083\n",
      "Validation loss per 100 evaluation steps: 0.013837600993637992\n",
      "Validation loss per 100 evaluation steps: 0.014902587732142624\n",
      "Validation loss per 100 evaluation steps: 0.01479345644707018\n",
      "Validation loss per 100 evaluation steps: 0.014680165659936443\n",
      "Validation loss per 100 evaluation steps: 0.014780680283064306\n",
      "Validation loss per 100 evaluation steps: 0.014856085340120905\n",
      "Validation loss per 100 evaluation steps: 0.01486986252035349\n",
      "Validation loss per 100 evaluation steps: 0.015051889051536411\n",
      "Validation loss per 100 evaluation steps: 0.01494179144375447\n",
      "Validation loss per 100 evaluation steps: 0.014956641187357515\n",
      "Validation loss per 100 evaluation steps: 0.015115703151048045\n",
      "Validation Loss: 0.01512171810137052\n",
      "Validation Accuracy: 0.9936327487459197\n",
      "Fold 0 validation ROC-AUC:  0.9981341732105931\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00   8977329\n",
      "           1       0.78      0.89      0.83    160335\n",
      "\n",
      "    accuracy                           0.99   9137664\n",
      "   macro avg       0.89      0.94      0.91   9137664\n",
      "weighted avg       0.99      0.99      0.99   9137664\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at 6-new-12w-0/ were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at 6-new-12w-0/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1 epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss per 1000 training steps: 0.9561225771903992\n",
      "Training loss per 1000 training steps: 0.3367122696558168\n",
      "Training loss per 1000 training steps: 0.18676853845261784\n",
      "Training loss epoch: 0.13229242627133278\n",
      "Training accuracy epoch: 0.9314306777398438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss per 100 evaluation steps: 0.010857054963707924\n",
      "Validation loss per 100 evaluation steps: 0.01877744071240915\n",
      "Validation loss per 100 evaluation steps: 0.01820546847105545\n",
      "Validation loss per 100 evaluation steps: 0.018374074002738695\n",
      "Validation loss per 100 evaluation steps: 0.018708943454240984\n",
      "Validation loss per 100 evaluation steps: 0.018623567883045172\n",
      "Validation loss per 100 evaluation steps: 0.018776613359394546\n",
      "Validation loss per 100 evaluation steps: 0.018794169456923193\n",
      "Validation loss per 100 evaluation steps: 0.018608136513644246\n",
      "Validation loss per 100 evaluation steps: 0.0184880892512157\n",
      "Validation loss per 100 evaluation steps: 0.018324344193032076\n",
      "Validation loss per 100 evaluation steps: 0.01825582644187183\n",
      "Validation Loss: 0.018237591461947363\n",
      "Validation Accuracy: 0.9920586167514721\n",
      "Fold 1 validation ROC-AUC:  0.9972987643842871\n",
      "Training fold 1 epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss per 1000 training steps: 0.026050148531794548\n",
      "Training loss per 1000 training steps: 0.018523255368107585\n",
      "Training loss per 1000 training steps: 0.017748052119403085\n",
      "Training loss epoch: 0.01713851108008354\n",
      "Training accuracy epoch: 0.9927929687499977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss per 100 evaluation steps: 0.00784144178032875\n",
      "Validation loss per 100 evaluation steps: 0.01579028967131704\n",
      "Validation loss per 100 evaluation steps: 0.015411798572353093\n",
      "Validation loss per 100 evaluation steps: 0.01559428440308681\n",
      "Validation loss per 100 evaluation steps: 0.01590047430080429\n",
      "Validation loss per 100 evaluation steps: 0.015893971111798313\n",
      "Validation loss per 100 evaluation steps: 0.01610129248790951\n",
      "Validation loss per 100 evaluation steps: 0.016145675902463463\n",
      "Validation loss per 100 evaluation steps: 0.015938536838957396\n",
      "Validation loss per 100 evaluation steps: 0.01578533728648345\n",
      "Validation loss per 100 evaluation steps: 0.01564028224983177\n",
      "Validation loss per 100 evaluation steps: 0.015567670338368424\n",
      "Validation Loss: 0.015547659287753257\n",
      "Validation Accuracy: 0.9934308451990927\n",
      "Fold 1 validation ROC-AUC:  0.9980211516496708\n",
      "Training fold 1 epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss per 1000 training steps: 0.02470959722995758\n",
      "Training loss per 1000 training steps: 0.015691776248439402\n",
      "Training loss per 1000 training steps: 0.015447855802257078\n",
      "Training loss epoch: 0.01520819717902737\n",
      "Training accuracy epoch: 0.9936630667892139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss per 100 evaluation steps: 0.00672173174098134\n",
      "Validation loss per 100 evaluation steps: 0.015980559419656155\n",
      "Validation loss per 100 evaluation steps: 0.01560603264274437\n",
      "Validation loss per 100 evaluation steps: 0.015887007658302928\n",
      "Validation loss per 100 evaluation steps: 0.01611660851424466\n",
      "Validation loss per 100 evaluation steps: 0.016126433715470342\n",
      "Validation loss per 100 evaluation steps: 0.016314842419739704\n",
      "Validation loss per 100 evaluation steps: 0.016361494614860108\n",
      "Validation loss per 100 evaluation steps: 0.01614822617037784\n",
      "Validation loss per 100 evaluation steps: 0.015979255341828983\n",
      "Validation loss per 100 evaluation steps: 0.015814416861443848\n",
      "Validation loss per 100 evaluation steps: 0.015734036353663213\n",
      "Validation Loss: 0.01571186211215043\n",
      "Validation Accuracy: 0.9934103907650089\n",
      "Fold 1 validation ROC-AUC:  0.9981050415115439\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00   8978661\n",
      "           1       0.76      0.91      0.83    159003\n",
      "\n",
      "    accuracy                           0.99   9137664\n",
      "   macro avg       0.88      0.95      0.91   9137664\n",
      "weighted avg       0.99      0.99      0.99   9137664\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at 6-new-12w-0/ were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at 6-new-12w-0/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 2 epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss per 1000 training steps: 0.6129879951477051\n",
      "Training loss per 1000 training steps: 0.21582649262355907\n",
      "Training loss per 1000 training steps: 0.12244308082261651\n",
      "Training loss epoch: 0.0889471043951559\n",
      "Training accuracy epoch: 0.9658031884628849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss per 100 evaluation steps: 0.01958499103784561\n",
      "Validation loss per 100 evaluation steps: 0.01698747275185098\n",
      "Validation loss per 100 evaluation steps: 0.01768353952567524\n",
      "Validation loss per 100 evaluation steps: 0.018574642480050904\n",
      "Validation loss per 100 evaluation steps: 0.018638680989219995\n",
      "Validation loss per 100 evaluation steps: 0.018715559989176764\n",
      "Validation loss per 100 evaluation steps: 0.01844183837546342\n",
      "Validation loss per 100 evaluation steps: 0.01839722794307265\n",
      "Validation loss per 100 evaluation steps: 0.018218243131908386\n",
      "Validation loss per 100 evaluation steps: 0.01825681163340753\n",
      "Validation loss per 100 evaluation steps: 0.018412178307936972\n",
      "Validation loss per 100 evaluation steps: 0.018381766099831835\n",
      "Validation Loss: 0.018337063570027155\n",
      "Validation Accuracy: 0.992160154499888\n",
      "Fold 2 validation ROC-AUC:  0.9971931745880414\n",
      "Training fold 2 epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss per 1000 training steps: 0.010685000568628311\n",
      "Training loss per 1000 training steps: 0.01770461489572413\n",
      "Training loss per 1000 training steps: 0.017190845102987608\n",
      "Training loss epoch: 0.01674443111108311\n",
      "Training accuracy epoch: 0.9930190826330573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss per 100 evaluation steps: 0.015171067789196968\n",
      "Validation loss per 100 evaluation steps: 0.015031831285980816\n",
      "Validation loss per 100 evaluation steps: 0.015673159092284776\n",
      "Validation loss per 100 evaluation steps: 0.016744584141784903\n",
      "Validation loss per 100 evaluation steps: 0.01687402656287921\n",
      "Validation loss per 100 evaluation steps: 0.01695153989016772\n",
      "Validation loss per 100 evaluation steps: 0.016714043817179502\n",
      "Validation loss per 100 evaluation steps: 0.016653179898445176\n",
      "Validation loss per 100 evaluation steps: 0.016487855314754073\n",
      "Validation loss per 100 evaluation steps: 0.016498588319333676\n",
      "Validation loss per 100 evaluation steps: 0.01666918242034745\n",
      "Validation loss per 100 evaluation steps: 0.016619302837934943\n",
      "Validation Loss: 0.016592364548094624\n",
      "Validation Accuracy: 0.9929995519713262\n",
      "Fold 2 validation ROC-AUC:  0.9978786018387729\n",
      "Training fold 2 epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss per 1000 training steps: 0.010346894152462482\n",
      "Training loss per 1000 training steps: 0.015000576828629687\n",
      "Training loss per 1000 training steps: 0.014906160081293423\n",
      "Training loss epoch: 0.014804722904051583\n",
      "Training accuracy epoch: 0.9938650264793419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss per 100 evaluation steps: 0.013719179667532444\n",
      "Validation loss per 100 evaluation steps: 0.014064276408828279\n",
      "Validation loss per 100 evaluation steps: 0.014656245524865292\n",
      "Validation loss per 100 evaluation steps: 0.01565944517814118\n",
      "Validation loss per 100 evaluation steps: 0.015769397854129147\n",
      "Validation loss per 100 evaluation steps: 0.0157968847303207\n",
      "Validation loss per 100 evaluation steps: 0.015526558087536333\n",
      "Validation loss per 100 evaluation steps: 0.015481949254745903\n",
      "Validation loss per 100 evaluation steps: 0.015325244765229078\n",
      "Validation loss per 100 evaluation steps: 0.01534046329294136\n",
      "Validation loss per 100 evaluation steps: 0.015500263001195525\n",
      "Validation loss per 100 evaluation steps: 0.015454577655414971\n",
      "Validation Loss: 0.015431994791225572\n",
      "Validation Accuracy: 0.9936437025719647\n",
      "Fold 2 validation ROC-AUC:  0.9980377957406961\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00   8975599\n",
      "           1       0.78      0.89      0.83    162065\n",
      "\n",
      "    accuracy                           0.99   9137664\n",
      "   macro avg       0.89      0.94      0.91   9137664\n",
      "weighted avg       0.99      0.99      0.99   9137664\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at 6-new-12w-0/ were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at 6-new-12w-0/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 3 epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss per 1000 training steps: 0.5684229731559753\n",
      "Training loss per 1000 training steps: 0.1985094067738294\n",
      "Training loss per 1000 training steps: 0.11482011681333694\n",
      "Training loss epoch: 0.08380284934140303\n",
      "Training accuracy epoch: 0.9700269990808797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss per 100 evaluation steps: 0.010321627371013165\n",
      "Validation loss per 100 evaluation steps: 0.0152081543220078\n",
      "Validation loss per 100 evaluation steps: 0.016438405443879485\n",
      "Validation loss per 100 evaluation steps: 0.016999731165785466\n",
      "Validation loss per 100 evaluation steps: 0.017084200352522082\n",
      "Validation loss per 100 evaluation steps: 0.016930920643868582\n",
      "Validation loss per 100 evaluation steps: 0.01705332129979475\n",
      "Validation loss per 100 evaluation steps: 0.01720707958769093\n",
      "Validation loss per 100 evaluation steps: 0.017335396803657963\n",
      "Validation loss per 100 evaluation steps: 0.017336445064457043\n",
      "Validation loss per 100 evaluation steps: 0.01734560162784275\n",
      "Validation loss per 100 evaluation steps: 0.01737211853293042\n",
      "Validation Loss: 0.017336527439488155\n",
      "Validation Accuracy: 0.9926932823700717\n",
      "Fold 3 validation ROC-AUC:  0.9972579518591168\n",
      "Training fold 3 epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss per 1000 training steps: 0.02048427425324917\n",
      "Training loss per 1000 training steps: 0.018337317932207328\n",
      "Training loss per 1000 training steps: 0.017607256968930153\n",
      "Training loss epoch: 0.017017875660343892\n",
      "Training accuracy epoch: 0.9928774947478985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss per 100 evaluation steps: 0.009032927453517914\n",
      "Validation loss per 100 evaluation steps: 0.013013076146668063\n",
      "Validation loss per 100 evaluation steps: 0.01419181709174317\n",
      "Validation loss per 100 evaluation steps: 0.014767286142232237\n",
      "Validation loss per 100 evaluation steps: 0.014947065631533854\n",
      "Validation loss per 100 evaluation steps: 0.01488020319371724\n",
      "Validation loss per 100 evaluation steps: 0.015085691661099762\n",
      "Validation loss per 100 evaluation steps: 0.015231597272661938\n",
      "Validation loss per 100 evaluation steps: 0.01536915640270549\n",
      "Validation loss per 100 evaluation steps: 0.015395110374322774\n",
      "Validation loss per 100 evaluation steps: 0.015398825601126615\n",
      "Validation loss per 100 evaluation steps: 0.015448468969394702\n",
      "Validation Loss: 0.01542629556237815\n",
      "Validation Accuracy: 0.9934470337351591\n",
      "Fold 3 validation ROC-AUC:  0.9980213745410325\n",
      "Training fold 3 epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss per 1000 training steps: 0.015300692059099674\n",
      "Training loss per 1000 training steps: 0.015508772670077374\n",
      "Training loss per 1000 training steps: 0.015277835655899748\n",
      "Training loss epoch: 0.015069317422971568\n",
      "Training accuracy epoch: 0.9937514771533624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss per 100 evaluation steps: 0.00866744201630354\n",
      "Validation loss per 100 evaluation steps: 0.012709079605370465\n",
      "Validation loss per 100 evaluation steps: 0.013796019048165919\n",
      "Validation loss per 100 evaluation steps: 0.01439679550425462\n",
      "Validation loss per 100 evaluation steps: 0.01462724275130458\n",
      "Validation loss per 100 evaluation steps: 0.014586242853534666\n",
      "Validation loss per 100 evaluation steps: 0.014812333171992731\n",
      "Validation loss per 100 evaluation steps: 0.01495463539606768\n",
      "Validation loss per 100 evaluation steps: 0.015084593754770994\n",
      "Validation loss per 100 evaluation steps: 0.015122077314490112\n",
      "Validation loss per 100 evaluation steps: 0.015119420292335548\n",
      "Validation loss per 100 evaluation steps: 0.015173005197460693\n",
      "Validation Loss: 0.015154109732988525\n",
      "Validation Accuracy: 0.993600824827789\n",
      "Fold 3 validation ROC-AUC:  0.998118192184717\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00   8980774\n",
      "           1       0.77      0.90      0.83    156890\n",
      "\n",
      "    accuracy                           0.99   9137664\n",
      "   macro avg       0.88      0.95      0.91   9137664\n",
      "weighted avg       0.99      0.99      0.99   9137664\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at 6-new-12w-0/ were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at 6-new-12w-0/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 4 epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss per 1000 training steps: 0.5960529446601868\n",
      "Training loss per 1000 training steps: 0.1973695021279\n",
      "Training loss per 1000 training steps: 0.11489072436399791\n",
      "Training loss epoch: 0.08388332206043689\n",
      "Training accuracy epoch: 0.9699559315913842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss per 100 evaluation steps: 0.002147783525288105\n",
      "Validation loss per 100 evaluation steps: 0.016507834325184916\n",
      "Validation loss per 100 evaluation steps: 0.016365828628498894\n",
      "Validation loss per 100 evaluation steps: 0.016936551633312605\n",
      "Validation loss per 100 evaluation steps: 0.017713038066745807\n",
      "Validation loss per 100 evaluation steps: 0.017757091088106153\n",
      "Validation loss per 100 evaluation steps: 0.017567596222024515\n",
      "Validation loss per 100 evaluation steps: 0.01758688510566307\n",
      "Validation loss per 100 evaluation steps: 0.017551238248915354\n",
      "Validation loss per 100 evaluation steps: 0.017486899718409662\n",
      "Validation loss per 100 evaluation steps: 0.017565205731042116\n",
      "Validation loss per 100 evaluation steps: 0.01756471906504006\n",
      "Validation Loss: 0.01751411867469439\n",
      "Validation Accuracy: 0.9926963294400841\n",
      "Fold 4 validation ROC-AUC:  0.997187239147049\n",
      "Training fold 4 epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss per 1000 training steps: 0.011222057044506073\n",
      "Training loss per 1000 training steps: 0.01823364680941839\n",
      "Training loss per 1000 training steps: 0.01753531229660857\n",
      "Training loss epoch: 0.016898507232065586\n",
      "Training accuracy epoch: 0.992917377888656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss per 100 evaluation steps: 0.0016201646067202091\n",
      "Validation loss per 100 evaluation steps: 0.014909034537262108\n",
      "Validation loss per 100 evaluation steps: 0.014593498250793906\n",
      "Validation loss per 100 evaluation steps: 0.015070625680631824\n",
      "Validation loss per 100 evaluation steps: 0.01587817675150708\n",
      "Validation loss per 100 evaluation steps: 0.015925356758488416\n",
      "Validation loss per 100 evaluation steps: 0.015731111580818604\n",
      "Validation loss per 100 evaluation steps: 0.01571350060209875\n",
      "Validation loss per 100 evaluation steps: 0.01569476370984129\n",
      "Validation loss per 100 evaluation steps: 0.015589301297217598\n",
      "Validation loss per 100 evaluation steps: 0.015642510812614925\n",
      "Validation loss per 100 evaluation steps: 0.015645761869450842\n",
      "Validation Loss: 0.015599308960409092\n",
      "Validation Accuracy: 0.9935076938674074\n",
      "Fold 4 validation ROC-AUC:  0.9980020043755723\n",
      "Training fold 4 epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss per 1000 training steps: 0.008467819541692734\n",
      "Training loss per 1000 training steps: 0.01546547696407006\n",
      "Training loss per 1000 training steps: 0.015230266071979593\n",
      "Training loss epoch: 0.014929415853126678\n",
      "Training accuracy epoch: 0.9938087031687672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss per 100 evaluation steps: 0.0015511962119489908\n",
      "Validation loss per 100 evaluation steps: 0.014979281107719877\n",
      "Validation loss per 100 evaluation steps: 0.014431537738572621\n",
      "Validation loss per 100 evaluation steps: 0.014873634499576611\n",
      "Validation loss per 100 evaluation steps: 0.015644719291906896\n",
      "Validation loss per 100 evaluation steps: 0.015691645911889162\n",
      "Validation loss per 100 evaluation steps: 0.01549973166505101\n",
      "Validation loss per 100 evaluation steps: 0.01551157870753728\n",
      "Validation loss per 100 evaluation steps: 0.015513116440482288\n",
      "Validation loss per 100 evaluation steps: 0.015422023960357412\n",
      "Validation loss per 100 evaluation steps: 0.015441342068006415\n",
      "Validation loss per 100 evaluation steps: 0.015428829430878461\n",
      "Validation Loss: 0.015380929382569047\n",
      "Validation Accuracy: 0.9934872081813235\n",
      "Fold 4 validation ROC-AUC:  0.998097415319662\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00   8977310\n",
      "           1       0.77      0.90      0.83    160354\n",
      "\n",
      "    accuracy                           0.99   9137664\n",
      "   macro avg       0.88      0.95      0.91   9137664\n",
      "weighted avg       0.99      0.99      0.99   9137664\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = 0\n",
    "lr = 1e-5\n",
    "EPOCHS = 3\n",
    "\n",
    "dir_to_pretrained_model = \"6-new-12w-0/\"\n",
    "config = BertConfig.from_pretrained('https://raw.githubusercontent.com/jerryji1993/DNABERT/master/src/transformers/dnabert-config/bert-config-6/config.json')\n",
    "tokenizer = DNATokenizer.from_pretrained('6-new-12w-0/')\n",
    "\n",
    "for MODEL_NUMBER in range(5):\n",
    "    train_dataset, test_dataset = load(f'ds_w_seq_hg_fold{MODEL_NUMBER}_kouzine.pkl')\n",
    "    training_loader = DataLoader(train_dataset, batch_size=24, num_workers = 2)\n",
    "    testing_loader = DataLoader(test_dataset, batch_size=16, num_workers = 2)\n",
    "\n",
    "\n",
    "    model = BertForTokenClassification.from_pretrained(dir_to_pretrained_model, config=config)\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, steps_per_epoch=len(training_loader), epochs=EPOCHS)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"Training fold {MODEL_NUMBER} epoch: {epoch + 1}\")\n",
    "        train(epoch)\n",
    "        labels, predictions, scores = valid(model, testing_loader)\n",
    "        print(f'Fold {MODEL_NUMBER} validation ROC-AUC: ', roc_auc_score(labels, scores))\n",
    "\n",
    "    print(sklearn.metrics.classification_report(labels, predictions))\n",
    "    model.save_pretrained(f'dnabert_hg_fold_{MODEL_NUMBER}_kouzine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [00:07<00:00,  3.34it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1054/1054 [00:11<00:00, 94.43it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 1054/1054 [00:00<00:00, 220940.40it/s]\n"
     ]
    }
   ],
   "source": [
    "chroms = [f'chr{i}' for i in list(range(1, 23)) + ['X', 'Y']]\n",
    "all_features = sorted([i[:-4] for i in os.listdir('../data/hg19_features/sparse/') if i.endswith('.pkl')])\n",
    "groups = ['DNase-seq', 'Histone', 'RNA polymerase', 'TFs and others']\n",
    "feature_names = [i for i in all_features if (i.split('_')[0] in groups)]\n",
    "ZDNA = load('ZDNA_hg19_kouzine.pkl')\n",
    "black_list = load(f'../data/hg19_zdna/sparse/blacklist_hg19.pkl')\n",
    "\n",
    "def chrom_reader(chrom):\n",
    "    files = sorted([i for i in os.listdir(f'../data/hg19_dna/') if f\"{chrom}_\" in i])\n",
    "    return ''.join([load(f\"../data/hg19_dna/{file}\") for file in files])\n",
    "\n",
    "DNA = {chrom:chrom_reader(chrom) for chrom in tqdm(chroms)}\n",
    "\n",
    "DNA_features = {feture: load(f'../data/hg19_features/sparse/{feture}.pkl')\n",
    "                for feture in tqdm(feature_names)}\n",
    "\n",
    "for feature in tqdm(DNA_features):\n",
    "    if set(DNA_features[feature].keys()) != set(chroms):\n",
    "        for chrom in chroms:\n",
    "            if chrom not in DNA_features[feature]:\n",
    "                DNA_features[feature][chrom] = SparseVector(len(DNA[chrom]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 0\n",
    "torch.cuda.empty_cache()\n",
    "models = []\n",
    "\n",
    "config = BertConfig.from_pretrained('https://raw.githubusercontent.com/jerryji1993/DNABERT/master/src/transformers/dnabert-config/bert-config-6/config.json')\n",
    "\n",
    "for MODEL_NUMBER in range(5):\n",
    "    dir_to_pretrained_model = f'dnabert_hg_fold_{MODEL_NUMBER}_kouzine'\n",
    "    model = BertForTokenClassification.from_pretrained(dir_to_pretrained_model, config=config).to(device)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'DNATokenizer'.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DNATokenizer.from_pretrained('6-new-12w-0/', add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 512\n",
    "k_mer_pad = 5\n",
    "\n",
    "def final_prediction(chrom):\n",
    "    \n",
    "    intervals = []\n",
    "    ends = []\n",
    "    \n",
    "    prediction = [np.zeros(ZDNA[chrom].shape, dtype=np.float32) for i in range(5)]\n",
    "    \n",
    "    \n",
    "    for st in range(0, ZDNA[chrom].shape[0] - width, width):\n",
    "        interval = [st, min(st + width + k_mer_pad, ZDNA[chrom].shape[0])]\n",
    "        intervals.append([chrom, interval[0], interval[1]])\n",
    "        \n",
    "    ends.append([chrom, ZDNA[chrom].shape[0] // width * width, ZDNA[chrom].shape[0]])\n",
    "\n",
    "\n",
    "    full_dataset = Dataset(chroms, [i for i in feature_names if i.upper() in intersect], \n",
    "                           DNA, DNA_features,  ZDNA, intervals, tokenizer)\n",
    "\n",
    "    ends_dataset = Dataset(chroms, [i for i in feature_names if i.upper() in intersect], \n",
    "                           DNA, DNA_features, ZDNA, ends, tokenizer)\n",
    "\n",
    "    params = {'batch_size':32, 'num_workers':5, 'shuffle':False}\n",
    "    load_predict = data.DataLoader(full_dataset, **params)\n",
    "\n",
    "    params = {'batch_size':1, 'num_workers':1, 'shuffle':False}\n",
    "    load_ends = data.DataLoader(ends_dataset, **params)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for features, labels, sequences, input_ids, intervals in load_predict:\n",
    "            input_ids = input_ids.to(device)\n",
    "            outputs = [torch.softmax(model(input_ids = input_ids)['logits'],axis = -1).cpu().numpy()[:,:,1] for model in models]\n",
    "            \n",
    "            for ind, interval in enumerate(zip(intervals[0], \n",
    "                                         intervals[1].cpu().numpy(),\n",
    "                                         intervals[2].cpu().numpy())):\n",
    "                #print(ind, interval)\n",
    "                #print(len(prediction), len(outputs), len(models))\n",
    "                for i in range(len(models)):\n",
    "                    prediction[i][interval[1]:interval[2]-k_mer_pad] = outputs[i][ind, :]\n",
    "                    \n",
    "    for model_i in range(len(models)):\n",
    "        dump(prediction[model_i], f'/gim/lv01/dumerenkov/zdna_data/new_mod_hg_{model_i}_{chrom}_kouzine', 3)\n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN CHROM chr8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN CHROM chr9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN CHROM chr10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN CHROM chr11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN CHROM chr12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN CHROM chr13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN CHROM chr14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "for chrom in chroms[7:]:\n",
    "    print(f\"BEGIN CHROM {chrom}\")\n",
    "    final_prediction(chrom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "equalized, divisions = load('hg_divisions_kouzine.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [06:05<00:00, 15.22s/it]\n"
     ]
    }
   ],
   "source": [
    "com_len = sum([len(DNA[chrom]) for chrom in chroms])\n",
    "sums = []\n",
    "\n",
    "for chrom in tqdm(chroms):\n",
    "    loc_sum = []\n",
    "    for model_num in range(5):\n",
    "        vec = load(f\"/gim/lv01/dumerenkov/zdna_data/new_mod_hg_{model_num}_{chrom}_kouzine\")\n",
    "        loc_sum.append(vec.sum())\n",
    "    sums.append(loc_sum)\n",
    "\n",
    "multipliers = np.array(sums).sum(axis=0) / com_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [4:55:39<00:00, 739.16s/it]\n"
     ]
    }
   ],
   "source": [
    "for chrom in tqdm(chroms):\n",
    "    vecs = np.array([load(f\"/gim/lv01/dumerenkov/zdna_data/new_mod_hg_{i}_{chrom}_kouzine\") \n",
    "                     for i in range(5)])\n",
    "    res_vec = (vecs / multipliers[:, None]) * multipliers.mean()\n",
    "    mean_vec = res_vec.mean(axis=0)\n",
    "    \n",
    "    test_ints = []\n",
    "    for MODEL_NUMBER in range(5):\n",
    "        train_inds, test_inds = divisions[MODEL_NUMBER]\n",
    "        train_intervals, test_intervals = [equalized[i] for i in train_inds], [equalized[i] for i in test_inds]\n",
    "        test_ints.extend([(MODEL_NUMBER, inter) for inter in test_intervals if inter[0] == chrom])\n",
    "    \n",
    "    for model_num, inters in test_ints:\n",
    "        mean_vec[inters[1]: inters[2]] = res_vec[model_num, inters[1]: inters[2]]\n",
    "    dump(mean_vec, f\"/gim/lv01/dumerenkov/zdna_data/new_mod_hg_res_{chrom}_kouzine\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 24/24 [1:25:14<00:00, 213.09s/it]\n"
     ]
    }
   ],
   "source": [
    "for chrom in tqdm(chroms):\n",
    "    vec = load(f\"/gim/lv01/dumerenkov/zdna_data/new_mod_hg_res_{chrom}_kouzine\")\n",
    "    iids = np.where(black_list[chrom].data == 1)[0]\n",
    "    for i, j in zip(black_list[chrom].indices[iids], black_list[chrom].indices[iids + 1]):\n",
    "        vec[i:j] = 0\n",
    "    dump(vec, f\"/gim/lv01/dumerenkov/zdna_data/new_mod_hg_res_{chrom}_kouzine\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
